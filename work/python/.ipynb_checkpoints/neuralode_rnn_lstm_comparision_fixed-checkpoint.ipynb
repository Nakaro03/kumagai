{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "osb4t4dd81q",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_technology_potential_visualization():\n",
    "    \"\"\"\n",
    "    æŠ€è¡“ãƒãƒ†ãƒ³ã‚·ãƒ£ãƒ«å ´ã¨ä¼æ¥­ã®æµã‚Œã‚’å¯è¦–åŒ–ã™ã‚‹ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³\n",
    "    æ—¢å­˜ã®VGAEãƒ¢ãƒ‡ãƒ«ã‹ã‚‰å¾—ã‚‰ã‚ŒãŸåŸ‹ã‚è¾¼ã¿ã‚’ä½¿ç”¨ã—ã¦å¯è¦–åŒ–ã‚’å®Ÿè¡Œ\n",
    "    \"\"\"\n",
    "    print(\"ğŸ¯ æŠ€è¡“ãƒãƒ†ãƒ³ã‚·ãƒ£ãƒ«å ´ã¨ä¼æ¥­æµã‚Œã®å¯è¦–åŒ–ãƒ‡ãƒ¢\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†\n",
    "    print(\"ğŸ“Š ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†...\")\n",
    "    cleaned_df = preprocess_data()\n",
    "    graphs, all_corps, all_patents, total_nodes = build_global_graphs(cleaned_df)\n",
    "    \n",
    "    if len(graphs) < 1:\n",
    "        print(\"å¯è¦–åŒ–ã«ã¯å°‘ãªãã¨ã‚‚1ã¤ã®å¹´ãƒ‡ãƒ¼ã‚¿ãŒå¿…è¦ã§ã™ã€‚\")\n",
    "        return\n",
    "        \n",
    "    # ä¸€ã¤ã®VGAEãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ï¼ˆNeuralODEã‚’ä½¿ç”¨ï¼‰\n",
    "    print(\"ğŸ”¬ VGAEï¼ˆNeuralODEï¼‰ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’...\")\n",
    "    model = UnifiedVGAE(\n",
    "        num_nodes=total_nodes,\n",
    "        num_corps=len(all_corps),\n",
    "        predictor_type='ode'\n",
    "    ).to(device)\n",
    "    \n",
    "    # çŸ­æ™‚é–“ã§å­¦ç¿’\n",
    "    history = train_model(model, graphs, len(all_corps), epochs=20)\n",
    "    \n",
    "    # æœ€æ–°å¹´ã®ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ã£ã¦åŸ‹ã‚è¾¼ã¿ã‚’å–å¾—\n",
    "    test_year = max(graphs.keys())\n",
    "    test_data = graphs[test_year].to(device)\n",
    "    \n",
    "    print(f\"ğŸ“ˆ {test_year}å¹´ã®ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ã£ã¦å¯è¦–åŒ–...\")\n",
    "    \n",
    "    # ãƒ¢ãƒ‡ãƒ«ã‹ã‚‰åŸ‹ã‚è¾¼ã¿ã‚’å–å¾—\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        node_indices = torch.arange(model.num_nodes)\n",
    "        z, mu, logvar = model.encode(test_data.x, test_data.edge_index, node_indices)\n",
    "        \n",
    "        # ä¼æ¥­ã¨ç‰¹è¨±ã®åŸ‹ã‚è¾¼ã¿ã‚’åˆ†é›¢\n",
    "        corp_embeddings = z[:len(all_corps)].cpu().numpy()\n",
    "        patent_embeddings = z[len(all_corps):].cpu().numpy()\n",
    "        \n",
    "        # ä¼æ¥­-ç‰¹è¨±æ¥ç¶šè¡Œåˆ—ã‚’æ§‹ç¯‰\n",
    "        edge_index = test_data.edge_index.cpu().numpy()\n",
    "        corp_patent_connections = np.zeros((len(all_corps), len(all_patents)))\n",
    "        \n",
    "        for i in range(edge_index.shape[1]):\n",
    "            corp_idx = edge_index[0, i]\n",
    "            patent_idx = edge_index[1, i] - len(all_corps)\n",
    "            if corp_idx < len(all_corps) and patent_idx >= 0 and patent_idx < len(all_patents):\n",
    "                corp_patent_connections[corp_idx, patent_idx] = 1\n",
    "    \n",
    "    # æŠ€è¡“å¯†åº¦å ´ã®è¨ˆç®—\n",
    "    print(\"ğŸ”¬ æŠ€è¡“å¯†åº¦å ´ã®è¨ˆç®—...\")\n",
    "    density_field, corp_2d, patent_2d, (x_grid, y_grid), pca = compute_technology_density(\n",
    "        corp_embeddings, patent_embeddings, corp_patent_connections, bandwidth=0.3\n",
    "    )\n",
    "    \n",
    "    # ãƒãƒ†ãƒ³ã‚·ãƒ£ãƒ«ã‚¨ãƒãƒ«ã‚®ãƒ¼å ´ã®è¨ˆç®—\n",
    "    print(\"âš¡ ãƒãƒ†ãƒ³ã‚·ãƒ£ãƒ«ã‚¨ãƒãƒ«ã‚®ãƒ¼å ´ã®è¨ˆç®—...\")\n",
    "    potential_field = compute_potential_energy_field(density_field, alpha=2.0)\n",
    "    \n",
    "    # å‹¾é…å ´ã®è¨ˆç®—\n",
    "    print(\"ğŸŒŠ å‹¾é…å ´ã®è¨ˆç®—...\")\n",
    "    grad_x, grad_y = compute_gradient_field(potential_field, x_grid, y_grid)\n",
    "    \n",
    "    # å¯è¦–åŒ–ã®å®Ÿè¡Œ\n",
    "    print(\"ğŸ¨ å¯è¦–åŒ–ã®å®Ÿè¡Œ...\")\n",
    "    fig = visualize_technology_potential_landscape(\n",
    "        density_field, potential_field, grad_x, grad_y,\n",
    "        corp_2d, patent_2d, x_grid, y_grid,\n",
    "        title=f\"æŠ€è¡“ãƒãƒ†ãƒ³ã‚·ãƒ£ãƒ«å ´ã¨ä¼æ¥­ã®æµã‚Œ ({test_year}å¹´)\"\n",
    "    )\n",
    "    \n",
    "    # ä¼æ¥­æµã‚Œã®å‹•åŠ›å­¦åˆ†æ\n",
    "    print(\"ğŸ“Š ä¼æ¥­æµã‚Œã®å‹•åŠ›å­¦åˆ†æ...\")\n",
    "    flow_analysis = analyze_corporate_flow_dynamics(\n",
    "        corp_2d, grad_x, grad_y, x_grid, y_grid, potential_field\n",
    "    )\n",
    "    \n",
    "    print(f\"\\\\nğŸ“ˆ åˆ†æçµæœ:\")\n",
    "    print(f\"â€¢ ä¼æ¥­æ•°: {len(corp_2d)}\")\n",
    "    print(f\"â€¢ ç‰¹è¨±æ•°: {len(patent_2d)}\")\n",
    "    print(f\"â€¢ å¹³å‡æµã‚Œå¼·åº¦: {flow_analysis['avg_flow_magnitude']:.4f}\")\n",
    "    print(f\"â€¢ æœ€å¤§æµã‚Œå¼·åº¦: {flow_analysis['max_flow_magnitude']:.4f}\")\n",
    "    \n",
    "    # å€‹åˆ¥ä¼æ¥­ã®æµã‚Œåˆ†æ\n",
    "    print(f\"\\\\nğŸ¢ å€‹åˆ¥ä¼æ¥­ã®æµã‚Œåˆ†æ:\")\n",
    "    for i, (pos, mag, direction, potential) in enumerate(zip(\n",
    "        flow_analysis['corp_positions'][:5],  # æœ€åˆã®5ä¼æ¥­ã®ã¿è¡¨ç¤º\n",
    "        flow_analysis['flow_magnitude'][:5],\n",
    "        flow_analysis['flow_direction'][:5],\n",
    "        flow_analysis['potential_at_corp'][:5]\n",
    "    )):\n",
    "        print(f\"  ä¼æ¥­{i+1}: ä½ç½®({pos[0]:.2f}, {pos[1]:.2f}), \"\n",
    "              f\"æµã‚Œå¼·åº¦={mag:.4f}, æ–¹å‘={direction:.1f}Â°, ãƒãƒ†ãƒ³ã‚·ãƒ£ãƒ«={potential:.2f}\")\n",
    "    \n",
    "    # å¯è¦–åŒ–ã®èª¬æ˜\n",
    "    print(f\"\\\\nğŸ” å¯è¦–åŒ–ã®èª¬æ˜:\")\n",
    "    print(\"â€¢ å·¦ä¸Š: æŠ€è¡“å¯†åº¦åˆ†å¸ƒï¼ˆç·‘è‰²ãŒé«˜å¯†åº¦ã€ç‰¹è¨±ã®é›†ä¸­é ˜åŸŸï¼‰\")\n",
    "    print(\"â€¢ å³ä¸Š: ãƒãƒ†ãƒ³ã‚·ãƒ£ãƒ«ã‚¨ãƒãƒ«ã‚®ãƒ¼å ´ï¼ˆèµ¤ã„é ˜åŸŸãŒä½ãƒãƒ†ãƒ³ã‚·ãƒ£ãƒ«ï¼æŠ€è¡“ãŒé›†ä¸­ï¼‰\")\n",
    "    print(\"â€¢ å·¦ä¸‹: å‹¾é…ãƒ™ã‚¯ãƒˆãƒ«å ´ï¼ˆçŸ¢å°ã¯ä¼æ¥­ã®ç§»å‹•æ–¹å‘ã€è‰²ã¯å‹¾é…ã®å¼·åº¦ï¼‰\")\n",
    "    print(\"â€¢ å³ä¸‹: çµ±åˆãƒ“ãƒ¥ãƒ¼ï¼ˆèµ¤ã„ç­‰é«˜ç·šã¯æŠ€è¡“ã®è°·ã€é’ã„çŸ¢å°ã¯ä¼æ¥­ã®æµã‚Œï¼‰\")\n",
    "    \n",
    "    return {\n",
    "        'model': model,\n",
    "        'density_field': density_field,\n",
    "        'potential_field': potential_field,\n",
    "        'grad_field': (grad_x, grad_y),\n",
    "        'positions': (corp_2d, patent_2d),\n",
    "        'flow_analysis': flow_analysis,\n",
    "        'year': test_year\n",
    "    }\n",
    "\n",
    "# ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã®å®Ÿè¡Œ\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"ğŸš€ æŠ€è¡“ãƒãƒ†ãƒ³ã‚·ãƒ£ãƒ«å ´å¯è¦–åŒ–ã‚·ã‚¹ãƒ†ãƒ ã®å®Ÿè¡Œ\")\n",
    "    result = demonstrate_technology_potential_visualization()\n",
    "    print(\"\\\\nâœ… å¯è¦–åŒ–å®Œäº†ï¼\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kzvpzbyazxm",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_technology_potential_landscape(density_field, potential_field, grad_x, grad_y, \n",
    "                                           corp_2d, patent_2d, x_grid, y_grid, \n",
    "                                           title=\"æŠ€è¡“ãƒãƒ†ãƒ³ã‚·ãƒ£ãƒ«å ´ã¨ä¼æ¥­ã®æµã‚Œ\"):\n",
    "    \"\"\"\n",
    "    æŠ€è¡“ãƒãƒ†ãƒ³ã‚·ãƒ£ãƒ«å ´ã¨ä¼æ¥­ã®æµã‚Œã‚’å¯è¦–åŒ–\n",
    "    \n",
    "    Args:\n",
    "        density_field: æŠ€è¡“å¯†åº¦å ´\n",
    "        potential_field: ãƒãƒ†ãƒ³ã‚·ãƒ£ãƒ«ã‚¨ãƒãƒ«ã‚®ãƒ¼å ´\n",
    "        grad_x, grad_y: å‹¾é…ãƒ™ã‚¯ãƒˆãƒ«å ´\n",
    "        corp_2d: ä¼æ¥­ã®2Dåº§æ¨™\n",
    "        patent_2d: ç‰¹è¨±ã®2Dåº§æ¨™\n",
    "        x_grid, y_grid: ã‚°ãƒªãƒƒãƒ‰ã®åº§æ¨™é…åˆ—\n",
    "        title: å›³ã®ã‚¿ã‚¤ãƒˆãƒ«\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    X, Y = np.meshgrid(x_grid, y_grid)\n",
    "    \n",
    "    # 1. æŠ€è¡“å¯†åº¦ã®å¯è¦–åŒ–\n",
    "    ax1 = axes[0, 0]\n",
    "    im1 = ax1.contourf(X, Y, density_field, levels=20, cmap='viridis', alpha=0.8)\n",
    "    ax1.scatter(patent_2d[:, 0], patent_2d[:, 1], c='red', s=30, alpha=0.7, label='ç‰¹è¨±')\n",
    "    ax1.scatter(corp_2d[:, 0], corp_2d[:, 1], c='blue', s=50, alpha=0.7, label='ä¼æ¥­', marker='s')\n",
    "    ax1.set_title('æŠ€è¡“å¯†åº¦åˆ†å¸ƒ', fontsize=14, fontweight='bold')\n",
    "    ax1.set_xlabel('PCAæ¬¡å…ƒ1')\n",
    "    ax1.set_ylabel('PCAæ¬¡å…ƒ2')\n",
    "    ax1.legend()\n",
    "    plt.colorbar(im1, ax=ax1, label='æŠ€è¡“å¯†åº¦')\n",
    "    \n",
    "    # 2. ãƒãƒ†ãƒ³ã‚·ãƒ£ãƒ«ã‚¨ãƒãƒ«ã‚®ãƒ¼å ´ã®å¯è¦–åŒ–\n",
    "    ax2 = axes[0, 1]\n",
    "    im2 = ax2.contourf(X, Y, potential_field, levels=20, cmap='RdYlBu_r', alpha=0.8)\n",
    "    contour_lines = ax2.contour(X, Y, potential_field, levels=10, colors='black', alpha=0.4, linewidths=0.8)\n",
    "    ax2.clabel(contour_lines, inline=True, fontsize=8)\n",
    "    ax2.scatter(corp_2d[:, 0], corp_2d[:, 1], c='blue', s=50, alpha=0.8, label='ä¼æ¥­', marker='s')\n",
    "    ax2.set_title('ãƒãƒ†ãƒ³ã‚·ãƒ£ãƒ«ã‚¨ãƒãƒ«ã‚®ãƒ¼å ´', fontsize=14, fontweight='bold')\n",
    "    ax2.set_xlabel('PCAæ¬¡å…ƒ1')\n",
    "    ax2.set_ylabel('PCAæ¬¡å…ƒ2')\n",
    "    ax2.legend()\n",
    "    plt.colorbar(im2, ax=ax2, label='ãƒãƒ†ãƒ³ã‚·ãƒ£ãƒ«')\n",
    "    \n",
    "    # 3. å‹¾é…ãƒ™ã‚¯ãƒˆãƒ«å ´ï¼ˆä¼æ¥­ã®æµã‚Œï¼‰\n",
    "    ax3 = axes[1, 0]\n",
    "    \n",
    "    # ãƒãƒ†ãƒ³ã‚·ãƒ£ãƒ«å ´ã‚’ç­‰é«˜ç·šã§è¡¨ç¤º\n",
    "    contour_bg = ax3.contourf(X, Y, potential_field, levels=15, cmap='RdYlBu_r', alpha=0.6)\n",
    "    \n",
    "    # å‹¾é…ãƒ™ã‚¯ãƒˆãƒ«å ´ã‚’çŸ¢å°ã§è¡¨ç¤ºï¼ˆé–“å¼•ã„ã¦è¡¨ç¤ºï¼‰\n",
    "    skip = 3  # çŸ¢å°ã®é–“å¼•ãé–“éš”\n",
    "    grad_magnitude = np.sqrt(grad_x**2 + grad_y**2)\n",
    "    \n",
    "    # å‹¾é…ã®å¤§ãã•ã§è‰²åˆ†ã‘\n",
    "    ax3.quiver(X[::skip, ::skip], Y[::skip, ::skip], \n",
    "               grad_x[::skip, ::skip], grad_y[::skip, ::skip],\n",
    "               grad_magnitude[::skip, ::skip], \n",
    "               cmap='plasma', scale=20, alpha=0.8, width=0.003)\n",
    "    \n",
    "    # ä¼æ¥­ä½ç½®ã‚’ãƒ—ãƒ­ãƒƒãƒˆ\n",
    "    ax3.scatter(corp_2d[:, 0], corp_2d[:, 1], c='white', s=60, alpha=0.9, \n",
    "                label='ä¼æ¥­', marker='s', edgecolors='black', linewidths=1)\n",
    "    \n",
    "    ax3.set_title('å‹¾é…å ´ï¼ˆä¼æ¥­ã®ç§»å‹•æ–¹å‘ï¼‰', fontsize=14, fontweight='bold')\n",
    "    ax3.set_xlabel('PCAæ¬¡å…ƒ1')\n",
    "    ax3.set_ylabel('PCAæ¬¡å…ƒ2')\n",
    "    ax3.legend()\n",
    "    \n",
    "    # 4. çµ±åˆãƒ“ãƒ¥ãƒ¼ï¼šç­‰é«˜ç·š + ä¼æ¥­ + æµã‚Œ\n",
    "    ax4 = axes[1, 1]\n",
    "    \n",
    "    # èƒŒæ™¯ã«æŠ€è¡“å¯†åº¦ã‚’è¡¨ç¤º\n",
    "    im4 = ax4.contourf(X, Y, density_field, levels=15, cmap='Greens', alpha=0.5)\n",
    "    \n",
    "    # ãƒãƒ†ãƒ³ã‚·ãƒ£ãƒ«ã®ç­‰é«˜ç·šï¼ˆèµ¤ã„è°·ï¼‰\n",
    "    contour_potential = ax4.contour(X, Y, potential_field, levels=12, colors='red', alpha=0.7, linewidths=1.5)\n",
    "    ax4.clabel(contour_potential, inline=True, fontsize=8, fmt='%.1f')\n",
    "    \n",
    "    # å‹¾é…ãƒ™ã‚¯ãƒˆãƒ«ï¼ˆé’ã„çŸ¢å°ã§ä¼æ¥­ã®ç§»å‹•æ–¹å‘ï¼‰\n",
    "    ax4.quiver(X[::skip, ::skip], Y[::skip, ::skip], \n",
    "               grad_x[::skip, ::skip], grad_y[::skip, ::skip],\n",
    "               color='blue', scale=15, alpha=0.7, width=0.004)\n",
    "    \n",
    "    # ä¼æ¥­ã¨ç‰¹è¨±ã®ä½ç½®\n",
    "    ax4.scatter(patent_2d[:, 0], patent_2d[:, 1], c='orange', s=25, alpha=0.6, label='ç‰¹è¨±')\n",
    "    ax4.scatter(corp_2d[:, 0], corp_2d[:, 1], c='navy', s=80, alpha=0.9, \n",
    "                label='ä¼æ¥­', marker='s', edgecolors='white', linewidths=1.5)\n",
    "    \n",
    "    ax4.set_title('çµ±åˆãƒ“ãƒ¥ãƒ¼ï¼šæŠ€è¡“ãƒãƒ†ãƒ³ã‚·ãƒ£ãƒ«å ´ã«ãŠã‘ã‚‹ä¼æ¥­ã®æµã‚Œ', fontsize=14, fontweight='bold')\n",
    "    ax4.set_xlabel('PCAæ¬¡å…ƒ1')\n",
    "    ax4.set_ylabel('PCAæ¬¡å…ƒ2')\n",
    "    ax4.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle(title, fontsize=16, fontweight='bold', y=1.02)\n",
    "    plt.show()\n",
    "    \n",
    "    return fig\n",
    "\n",
    "def analyze_corporate_flow_dynamics(corp_2d, grad_x, grad_y, x_grid, y_grid, potential_field):\n",
    "    \"\"\"\n",
    "    ä¼æ¥­ã®æµã‚Œã®å‹•åŠ›å­¦ã‚’åˆ†æ\n",
    "    \n",
    "    Args:\n",
    "        corp_2d: ä¼æ¥­ã®2Dåº§æ¨™\n",
    "        grad_x, grad_y: å‹¾é…ãƒ™ã‚¯ãƒˆãƒ«å ´\n",
    "        x_grid, y_grid: ã‚°ãƒªãƒƒãƒ‰ã®åº§æ¨™é…åˆ—\n",
    "        potential_field: ãƒãƒ†ãƒ³ã‚·ãƒ£ãƒ«å ´\n",
    "        \n",
    "    Returns:\n",
    "        flow_analysis: æµã‚Œã®åˆ†æçµæœ\n",
    "    \"\"\"\n",
    "    from scipy.interpolate import griddata\n",
    "    \n",
    "    # ä¼æ¥­ä½ç½®ã§ã®å‹¾é…ã‚’è£œé–“\n",
    "    X, Y = np.meshgrid(x_grid, y_grid)\n",
    "    grid_points = np.column_stack([X.ravel(), Y.ravel()])\n",
    "    grad_x_flat = grad_x.ravel()\n",
    "    grad_y_flat = grad_y.ravel()\n",
    "    \n",
    "    corp_grad_x = griddata(grid_points, grad_x_flat, corp_2d, method='linear', fill_value=0)\n",
    "    corp_grad_y = griddata(grid_points, grad_y_flat, corp_2d, method='linear', fill_value=0)\n",
    "    \n",
    "    # ä¼æ¥­ä½ç½®ã§ã®ãƒãƒ†ãƒ³ã‚·ãƒ£ãƒ«å€¤\n",
    "    potential_flat = potential_field.ravel()\n",
    "    corp_potential = griddata(grid_points, potential_flat, corp_2d, method='linear', fill_value=0)\n",
    "    \n",
    "    # æµã‚Œã®å¼·åº¦ã¨æ–¹å‘\n",
    "    flow_magnitude = np.sqrt(corp_grad_x**2 + corp_grad_y**2)\n",
    "    flow_direction = np.arctan2(corp_grad_y, corp_grad_x) * 180 / np.pi\n",
    "    \n",
    "    analysis = {\n",
    "        'corp_positions': corp_2d,\n",
    "        'flow_vectors': np.column_stack([corp_grad_x, corp_grad_y]),\n",
    "        'flow_magnitude': flow_magnitude,\n",
    "        'flow_direction': flow_direction,\n",
    "        'potential_at_corp': corp_potential,\n",
    "        'avg_flow_magnitude': np.mean(flow_magnitude),\n",
    "        'max_flow_magnitude': np.max(flow_magnitude)\n",
    "    }\n",
    "    \n",
    "    return analysis\n",
    "\n",
    "print(\"å¯è¦–åŒ–ã¨ãƒ•ãƒ­ãƒ¼åˆ†æé–¢æ•°ã‚’å®šç¾©ã—ã¾ã—ãŸ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1nkzi25hgy8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import KernelDensity\n",
    "import seaborn as sns\n",
    "import japanize_matplotlib\n",
    "\n",
    "def compute_technology_density(corp_embeddings, patent_embeddings, corp_patent_connections, bandwidth=0.5):\n",
    "    \"\"\"\n",
    "    æŠ€è¡“ãƒˆãƒ”ãƒƒã‚¯ã®å¯†åº¦ã‚’è¨ˆç®—ã™ã‚‹\n",
    "    \n",
    "    Args:\n",
    "        corp_embeddings: ä¼æ¥­ã®åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ« (N_corp x dim)\n",
    "        patent_embeddings: ç‰¹è¨±ã®åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ« (N_patent x dim)\n",
    "        corp_patent_connections: ä¼æ¥­-ç‰¹è¨±æ¥ç¶šè¡Œåˆ— (N_corp x N_patent)\n",
    "        bandwidth: ã‚«ãƒ¼ãƒãƒ«å¯†åº¦æ¨å®šã®ãƒãƒ³ãƒ‰å¹…\n",
    "    \n",
    "    Returns:\n",
    "        density_field: 2Då¯†åº¦å ´ (grid_size x grid_size)\n",
    "        x_range, y_range: ã‚°ãƒªãƒƒãƒ‰ã®ç¯„å›²\n",
    "    \"\"\"\n",
    "    # PCAã§2æ¬¡å…ƒã«æŠ•å½±\n",
    "    all_embeddings = np.vstack([corp_embeddings, patent_embeddings])\n",
    "    pca = PCA(n_components=2, random_state=42)\n",
    "    all_2d = pca.fit_transform(all_embeddings)\n",
    "    \n",
    "    corp_2d = all_2d[:len(corp_embeddings)]\n",
    "    patent_2d = all_2d[len(corp_embeddings):]\n",
    "    \n",
    "    # ç‰¹è¨±ã®é‡ã¿ã‚’ä¼æ¥­ã¨ã®æ¥ç¶šæ•°ã§è¨ˆç®—\n",
    "    patent_weights = np.sum(corp_patent_connections, axis=0)\n",
    "    patent_weights = patent_weights / np.max(patent_weights) if np.max(patent_weights) > 0 else patent_weights\n",
    "    \n",
    "    # ã‚°ãƒªãƒƒãƒ‰ä½œæˆ\n",
    "    grid_size = 50\n",
    "    x_min, x_max = np.min(all_2d[:, 0]) - 1, np.max(all_2d[:, 0]) + 1\n",
    "    y_min, y_max = np.min(all_2d[:, 1]) - 1, np.max(all_2d[:, 1]) + 1\n",
    "    \n",
    "    x_grid = np.linspace(x_min, x_max, grid_size)\n",
    "    y_grid = np.linspace(y_min, y_max, grid_size)\n",
    "    X, Y = np.meshgrid(x_grid, y_grid)\n",
    "    grid_points = np.column_stack([X.ravel(), Y.ravel()])\n",
    "    \n",
    "    # ã‚«ãƒ¼ãƒãƒ«å¯†åº¦æ¨å®šã§æŠ€è¡“å¯†åº¦ã‚’è¨ˆç®—\n",
    "    kde = KernelDensity(bandwidth=bandwidth, kernel='gaussian')\n",
    "    \n",
    "    # ç‰¹è¨±ã®é‡ã¿ä»˜ãå¯†åº¦\n",
    "    if len(patent_2d) > 0:\n",
    "        kde.fit(patent_2d, sample_weight=patent_weights)\n",
    "        log_density = kde.score_samples(grid_points)\n",
    "        density_field = np.exp(log_density).reshape(grid_size, grid_size)\n",
    "    else:\n",
    "        density_field = np.zeros((grid_size, grid_size))\n",
    "    \n",
    "    return density_field, corp_2d, patent_2d, (x_grid, y_grid), pca\n",
    "\n",
    "def compute_potential_energy_field(density_field, alpha=1.0):\n",
    "    \"\"\"\n",
    "    æŠ€è¡“å¯†åº¦ã‹ã‚‰ãƒãƒ†ãƒ³ã‚·ãƒ£ãƒ«ã‚¨ãƒãƒ«ã‚®ãƒ¼å ´ã‚’è¨ˆç®—\n",
    "    \n",
    "    Args:\n",
    "        density_field: æŠ€è¡“å¯†åº¦å ´\n",
    "        alpha: ãƒãƒ†ãƒ³ã‚·ãƒ£ãƒ«ã®å¼·åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
    "    \n",
    "    Returns:\n",
    "        potential_field: ãƒãƒ†ãƒ³ã‚·ãƒ£ãƒ«ã‚¨ãƒãƒ«ã‚®ãƒ¼å ´\n",
    "    \"\"\"\n",
    "    # å¯†åº¦ã®é€†æ•°ã‚’ãƒãƒ†ãƒ³ã‚·ãƒ£ãƒ«ã¨ã—ã¦ä½¿ç”¨ï¼ˆå¯†åº¦ãŒé«˜ã„æ‰€ãŒã‚¨ãƒãƒ«ã‚®ãƒ¼ãŒä½ã„ï¼‰\n",
    "    # å¯¾æ•°å¤‰æ›ã§ã‚¹ã‚±ãƒ¼ãƒ«ã‚’èª¿æ•´\n",
    "    potential_field = -alpha * np.log(density_field + 1e-10)\n",
    "    \n",
    "    # ã‚¬ã‚¦ã‚·ã‚¢ãƒ³ãƒ•ã‚£ãƒ«ã‚¿ã§ã‚¹ãƒ ãƒ¼ã‚¸ãƒ³ã‚°\n",
    "    potential_field = gaussian_filter(potential_field, sigma=1.0)\n",
    "    \n",
    "    return potential_field\n",
    "\n",
    "def compute_gradient_field(potential_field, x_grid, y_grid):\n",
    "    \"\"\"\n",
    "    ãƒãƒ†ãƒ³ã‚·ãƒ£ãƒ«å ´ã‹ã‚‰å‹¾é…å ´ã‚’è¨ˆç®—\n",
    "    \n",
    "    Args:\n",
    "        potential_field: ãƒãƒ†ãƒ³ã‚·ãƒ£ãƒ«ã‚¨ãƒãƒ«ã‚®ãƒ¼å ´\n",
    "        x_grid, y_grid: ã‚°ãƒªãƒƒãƒ‰ã®åº§æ¨™é…åˆ—\n",
    "    \n",
    "    Returns:\n",
    "        grad_x, grad_y: å‹¾é…ãƒ™ã‚¯ãƒˆãƒ«å ´ã®x, yæˆåˆ†\n",
    "    \"\"\"\n",
    "    dx = x_grid[1] - x_grid[0]\n",
    "    dy = y_grid[1] - y_grid[0]\n",
    "    \n",
    "    # å‹¾é…è¨ˆç®—ï¼ˆä¸­å¤®å·®åˆ†ï¼‰\n",
    "    grad_y, grad_x = np.gradient(potential_field, dy, dx)\n",
    "    \n",
    "    # å‹¾é…ã®æ–¹å‘ã‚’åè»¢ï¼ˆåŠ›ã®æ–¹å‘ï¼‰\n",
    "    grad_x = -grad_x\n",
    "    grad_y = -grad_y\n",
    "    \n",
    "    return grad_x, grad_y\n",
    "\n",
    "print(\"æŠ€è¡“å¯†åº¦ã¨ãƒãƒ†ãƒ³ã‚·ãƒ£ãƒ«å ´è¨ˆç®—é–¢æ•°ã‚’å®šç¾©ã—ã¾ã—ãŸ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ac781d",
   "metadata": {},
   "source": [
    "### NeuralODE vs RNN vs LSTM ãƒ¢ãƒ‡ãƒ«ã®æ¯”è¼ƒè©•ä¾¡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b023316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "1. Loading and preprocessing data...\n",
      "Data loaded: 4777 corporations, 49262 patents, 33 yearly graphs.\n",
      "\n",
      "2. Starting model training and evaluation...\n",
      "\n",
      "--- Training VGAE+ODE ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training VGAE+ODE:   0%|          | 0/50 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "LinkLoader.__init__() missing 1 required positional argument: 'link_sampler'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[7], line 420\u001b[0m\n",
      "\u001b[1;32m    416\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results, histories\n",
      "\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;32m--> 420\u001b[0m     results, histories \u001b[38;5;241m=\u001b[39m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "Cell \u001b[0;32mIn[7], line 383\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n",
      "\u001b[1;32m    381\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, model \u001b[38;5;129;01min\u001b[39;00m models_to_compare\u001b[38;5;241m.\u001b[39mitems():\n",
      "\u001b[1;32m    382\u001b[0m     start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "\u001b[0;32m--> 383\u001b[0m     trained_model, history, best_val_auc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgraph_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_corps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    384\u001b[0m     training_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n",
      "\u001b[1;32m    385\u001b[0m     histories[name] \u001b[38;5;241m=\u001b[39m history\n",
      "\n",
      "Cell \u001b[0;32mIn[7], line 246\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, graph_dict, num_corps, model_name, num_epochs)\u001b[0m\n",
      "\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m data_t\u001b[38;5;241m.\u001b[39medge_index\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m: \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[1;32m    245\u001b[0m \u001b[38;5;66;03m# â–¼â–¼â–¼ å¤ã„ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã®LinkLoaderã®æ­£ã—ã„ä½¿ã„æ–¹ã«ä¿®æ­£ â–¼â–¼â–¼\u001b[39;00m\n",
      "\u001b[0;32m--> 246\u001b[0m loader \u001b[38;5;241m=\u001b[39m \u001b[43mLinkLoader\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[1;32m    247\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_t\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    248\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_neighbors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    249\u001b[0m \u001b[43m    \u001b[49m\u001b[43mneg_sampling_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    250\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2048\u001b[39;49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    251\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# num_workersã¯ç’°å¢ƒã«ã‚ˆã£ã¦èª¿æ•´\u001b[39;49;00m\n",
      "\u001b[1;32m    253\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    255\u001b[0m \u001b[38;5;66;03m# äºˆæ¸¬æå¤±è¨ˆç®—ã®ãŸã‚ã«ã€t+1ã®ã‚°ãƒ©ãƒ•å…¨ä½“ã®æ½œåœ¨è¡¨ç¾ã‚’äº‹å‰ã«è¨ˆç®—\u001b[39;00m\n",
      "\u001b[1;32m    256\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "\n",
      "\u001b[0;31mTypeError\u001b[0m: LinkLoader.__init__() missing 1 required positional argument: 'link_sampler'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.loader import LinkLoader\n",
    "from torchdiffeq import odeint\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import japanize_matplotlib\n",
    "import gc\n",
    "import os\n",
    "\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# å®šæ•°å®šç¾©\n",
    "# ==============================================================================\n",
    "# ç¾åœ¨ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«åŸºã¥ã„ã¦ãƒ‘ã‚¹ã‚’è¨­å®š\n",
    "current_dir = os.getcwd()\n",
    "if 'work' in current_dir:\n",
    "    base_path = current_dir\n",
    "else:\n",
    "    base_path = os.path.join(current_dir, 'work')\n",
    "\n",
    "KG_DATASET_PATH = os.path.join(base_path, \"dataset\", \"kg_dataset.csv\")\n",
    "PATENT_DATA_PATH = os.path.join(base_path, \"processed_data\", \"kumagai_patentdata.csv\")\n",
    "\n",
    "print(f\"KG Dataset path: {KG_DATASET_PATH}\")\n",
    "print(f\"Patent data path: {PATENT_DATA_PATH}\")\n",
    "print(f\"Files exist - KG: {os.path.exists(KG_DATASET_PATH)}, Patent: {os.path.exists(PATENT_DATA_PATH)}\")\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç† (å¤‰æ›´ãªã—)\n",
    "# ==============================================================================\n",
    "def preprocess_data(kg_path=KG_DATASET_PATH, patent_path=PATENT_DATA_PATH):\n",
    "    \"\"\"\n",
    "    2ã¤ã®CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã€ãƒãƒ¼ã‚¸ã—ã¦å‰å‡¦ç†ã‚’è¡Œã†ã€‚\n",
    "    ç‰¹è¨±ã¨ä¼æ¥­ã®æ™‚ç³»åˆ—ã‚°ãƒ©ãƒ•æ§‹ç¯‰ã«å¿…è¦ãªãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’è¿”ã™ã€‚\n",
    "ã›    \"\"\"\n",
    "    try:\n",
    "        print(f\"Loading KG dataset from: {kg_path}\")\n",
    "        kg_df = pd.read_csv(kg_path)\n",
    "        print(f\"KG dataset shape: {kg_df.shape}\")\n",
    "        \n",
    "        print(f\"Loading patent data from: {patent_path}\")\n",
    "        kumagai_df = pd.read_csv(patent_path)\n",
    "        print(f\"Patent data shape: {kumagai_df.shape}\")\n",
    "        \n",
    "        merged_df = pd.merge(kg_df, kumagai_df, on=\"patent_number\", how=\"left\")\n",
    "        print(f\"Merged data shape: {merged_df.shape}\")\n",
    "        \n",
    "        cleaned_df = merged_df.dropna(subset=[\"corporation\", \"patent_number\"], how=\"any\")\n",
    "        print(f\"Cleaned data shape: {cleaned_df.shape}\")\n",
    "        \n",
    "        feature_cols = [f\"pca_text_dim_{i}\" for i in range(64)] + [f\"node_dim_{i}\" for i in range(64)]\n",
    "        # å­˜åœ¨ã™ã‚‹ã‚«ãƒ©ãƒ ã®ã¿ã‚’å‡¦ç†\n",
    "        existing_feature_cols = [col for col in feature_cols if col in cleaned_df.columns]\n",
    "        print(f\"Found {len(existing_feature_cols)} feature columns out of {len(feature_cols)} expected\")\n",
    "        \n",
    "        if existing_feature_cols:\n",
    "            cleaned_df[existing_feature_cols] = cleaned_df[existing_feature_cols].apply(pd.to_numeric, errors='coerce').replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "            cleaned_df[existing_feature_cols] = cleaned_df[existing_feature_cols].astype(np.float32)\n",
    "        \n",
    "        def safe_literal_eval(s):\n",
    "            try: \n",
    "                return ast.literal_eval(s)\n",
    "            except (ValueError, SyntaxError): \n",
    "                return []\n",
    "        \n",
    "        cleaned_df[\"corporation\"] = cleaned_df[\"corporation\"].apply(safe_literal_eval)\n",
    "        cleaned_df['year_month'] = pd.to_datetime(cleaned_df['year_month'], format='%Y-%m')\n",
    "        \n",
    "        return cleaned_df, existing_feature_cols\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in preprocess_data: {e}\")\n",
    "        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ã®ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’è¿”ã™\n",
    "        return pd.DataFrame(), []\n",
    "\n",
    "def build_global_graphs(df, feature_cols):\n",
    "    \"\"\"\n",
    "    ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‹ã‚‰å¹´ã”ã¨ã®ã‚°ãƒ©ãƒ•ãƒ‡ãƒ¼ã‚¿ã‚’æ§‹ç¯‰ã™ã‚‹ã€‚\n",
    "    å…¨ã¦ã®ãƒãƒ¼ãƒ‰ï¼ˆä¼æ¥­ãƒ»ç‰¹è¨±ï¼‰ã‚’å«ã‚€ã‚°ãƒ­ãƒ¼ãƒãƒ«ãªã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ç©ºé–“ã§ã‚°ãƒ©ãƒ•ã‚’ä½œæˆã™ã‚‹ã€‚\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        print(\"Warning: Empty dataframe provided to build_global_graphs\")\n",
    "        return {}, [], [], 0\n",
    "    \n",
    "    year_groups = df.groupby(df['year_month'].dt.year)\n",
    "    all_corporations = sorted(list(set(c for corps in df['corporation'] for c in corps)))\n",
    "    all_patents = sorted(list(df['patent_number'].unique()))\n",
    "    corp_to_idx = {corp: i for i, corp in enumerate(all_corporations)}\n",
    "    patent_to_idx = {patent: i + len(all_corporations) for i, patent in enumerate(all_patents)}\n",
    "    num_total_nodes = len(all_corporations) + len(all_patents)\n",
    "\n",
    "    print(f\"Building graphs: {len(all_corporations)} corporations, {len(all_patents)} patents, {num_total_nodes} total nodes\")\n",
    "\n",
    "    graph_dict = {}\n",
    "    for year, group in year_groups:\n",
    "        edges = []\n",
    "        for _, row in group.iterrows():\n",
    "            for corp in row['corporation']:\n",
    "                if corp in corp_to_idx and row['patent_number'] in patent_to_idx:\n",
    "                    edges.append([corp_to_idx[corp], patent_to_idx[row['patent_number']]])\n",
    "        if not edges: \n",
    "            print(f\"No edges found for year {year}, skipping\")\n",
    "            continue\n",
    "\n",
    "        # ç‰¹å¾´é‡ã®æ¬¡å…ƒã‚’å‹•çš„ã«æ±ºå®š\n",
    "        feature_dim = len(feature_cols) if feature_cols else 128\n",
    "        x = torch.randn(num_total_nodes, feature_dim) # ç‰¹å¾´é‡ã¯ãƒãƒ¼ãƒ‰å…¨ä½“ã§å…±æœ‰\n",
    "        \n",
    "        for _, row in group.iterrows():\n",
    "            if row['patent_number'] in patent_to_idx:\n",
    "                patent_idx = patent_to_idx[row['patent_number']]\n",
    "                if feature_cols:\n",
    "                    # åˆ©ç”¨å¯èƒ½ãªç‰¹å¾´é‡ã‚’ä½¿ç”¨\n",
    "                    available_features = [col for col in feature_cols if col in row.index]\n",
    "                    if available_features:\n",
    "                        feat_values = row[available_features].values.astype(np.float32)\n",
    "                        # ç‰¹å¾´é‡ã®æ¬¡å…ƒã‚’åˆã‚ã›ã‚‹\n",
    "                        if len(feat_values) < feature_dim:\n",
    "                            feat_values = np.pad(feat_values, (0, feature_dim - len(feat_values)), 'constant')\n",
    "                        elif len(feat_values) > feature_dim:\n",
    "                            feat_values = feat_values[:feature_dim]\n",
    "                        x[patent_idx] = torch.tensor(feat_values, dtype=torch.float32)\n",
    "\n",
    "        edge_index = torch.tensor(edges, dtype=torch.long).t()\n",
    "        graph_dict[year] = Data(x=x, edge_index=edge_index, num_nodes=num_total_nodes, year=year)\n",
    "        print(f\"Year {year}: {edge_index.size(1)} edges\")\n",
    "\n",
    "    return graph_dict, all_corporations, all_patents, num_total_nodes\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# ãƒ¢ãƒ‡ãƒ«å®šç¾© (å¤‰æ›´ãªã—)\n",
    "# ==============================================================================\n",
    "class SharedVGAEEncoder(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels * 2)\n",
    "        self.conv2 = GCNConv(hidden_channels * 2, hidden_channels)\n",
    "        self.conv3 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.conv_mu = GCNConv(hidden_channels, out_channels)\n",
    "        self.conv_logvar = GCNConv(hidden_channels, out_channels)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.batch_norm1 = nn.BatchNorm1d(hidden_channels * 2)\n",
    "        self.batch_norm2 = nn.BatchNorm1d(hidden_channels)\n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.relu(self.batch_norm1(self.conv1(x, edge_index)))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.batch_norm2(self.conv2(x, edge_index)))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.conv3(x, edge_index))\n",
    "        return self.conv_mu(x, edge_index), self.conv_logvar(x, edge_index)\n",
    "\n",
    "class ODEFunc(nn.Module):\n",
    "    def __init__(self, latent_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(nn.Linear(latent_dim, hidden_dim), nn.Tanh(), nn.Linear(hidden_dim, hidden_dim), nn.Tanh(), nn.Linear(hidden_dim, latent_dim))\n",
    "    def forward(self, t, z): return self.net(z)\n",
    "\n",
    "class NeuralODEPredictor(nn.Module):\n",
    "    def __init__(self, latent_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.ode_func = ODEFunc(latent_dim, hidden_dim)\n",
    "    def forward(self, z_current, delta_t=1.0):\n",
    "        t_span = torch.tensor([0., delta_t], device=z_current.device)\n",
    "        return odeint(self.ode_func, z_current, t_span, method='dopri5')[-1]\n",
    "\n",
    "class RNNPredictor(nn.Module):\n",
    "    def __init__(self, latent_dim, hidden_dim, num_layers=2):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.RNN(latent_dim, hidden_dim, num_layers, batch_first=True, dropout=0.2)\n",
    "        self.output_layer = nn.Linear(hidden_dim, latent_dim)\n",
    "    def forward(self, z_sequence):\n",
    "        if z_sequence.dim() == 2: z_sequence = z_sequence.unsqueeze(1)\n",
    "        rnn_out, _ = self.rnn(z_sequence)\n",
    "        return self.output_layer(rnn_out[:, -1, :])\n",
    "\n",
    "class LSTMPredictor(nn.Module):\n",
    "    def __init__(self, latent_dim, hidden_dim, num_layers=2):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(latent_dim, hidden_dim, num_layers, batch_first=True, dropout=0.2)\n",
    "        self.output_layer = nn.Linear(hidden_dim, latent_dim)\n",
    "    def forward(self, z_sequence):\n",
    "        if z_sequence.dim() == 2: z_sequence = z_sequence.unsqueeze(1)\n",
    "        lstm_out, _ = self.lstm(z_sequence)\n",
    "        return self.output_layer(lstm_out[:, -1, :])\n",
    "\n",
    "class UnifiedVGAE(nn.Module):\n",
    "    def __init__(self, num_nodes, num_corps, input_dim=128, hidden_dim=64, latent_dim=16, predictor_type='ode'):\n",
    "        super().__init__()\n",
    "        self.num_nodes = num_nodes\n",
    "        self.num_corps = num_corps\n",
    "        self.input_dim = input_dim\n",
    "        self.corp_embeddings = nn.Embedding(num_corps, input_dim)\n",
    "        nn.init.xavier_uniform_(self.corp_embeddings.weight)\n",
    "        self.encoder = SharedVGAEEncoder(input_dim, hidden_dim, latent_dim)\n",
    "        if predictor_type == 'ode': self.temporal_predictor = NeuralODEPredictor(latent_dim, hidden_dim)\n",
    "        elif predictor_type == 'rnn': self.temporal_predictor = RNNPredictor(latent_dim, hidden_dim)\n",
    "        elif predictor_type == 'lstm': self.temporal_predictor = LSTMPredictor(latent_dim, hidden_dim)\n",
    "        else: raise ValueError(f\"Unknown predictor type: {predictor_type}\")\n",
    "        self.link_predictor = nn.Sequential(nn.Linear(latent_dim * 2, hidden_dim), nn.ReLU(), nn.Dropout(0.3), nn.Linear(hidden_dim, hidden_dim // 2), nn.ReLU(), nn.Linear(hidden_dim // 2, 1))\n",
    "    def get_node_features(self, x, node_indices):\n",
    "        features = x.clone()\n",
    "        corp_mask = node_indices < self.num_corps\n",
    "        if corp_mask.any():\n",
    "            corp_indices = node_indices[corp_mask]\n",
    "            # ç‰¹å¾´é‡ã®æ¬¡å…ƒã‚’åˆã‚ã›ã‚‹\n",
    "            corp_embeddings = self.corp_embeddings(corp_indices)\n",
    "            if corp_embeddings.size(1) != features.size(1):\n",
    "                # æ¬¡å…ƒãŒåˆã‚ãªã„å ´åˆã¯èª¿æ•´\n",
    "                if corp_embeddings.size(1) < features.size(1):\n",
    "                    padding = torch.zeros(corp_embeddings.size(0), features.size(1) - corp_embeddings.size(1), device=corp_embeddings.device)\n",
    "                    corp_embeddings = torch.cat([corp_embeddings, padding], dim=1)\n",
    "                else:\n",
    "                    corp_embeddings = corp_embeddings[:, :features.size(1)]\n",
    "            features[corp_mask] = corp_embeddings\n",
    "        return features\n",
    "    def encode(self, x, edge_index, node_indices):\n",
    "        x_featured = self.get_node_features(x, node_indices)\n",
    "        mu, logvar = self.encoder(x_featured, edge_index.long())\n",
    "        if self.training:\n",
    "            std = torch.exp(0.5 * logvar)\n",
    "            eps = torch.randn_like(std)\n",
    "            return mu + eps * std, mu, logvar\n",
    "        return mu, mu, logvar\n",
    "    def decode(self, z, edge_index):\n",
    "        edge_index = edge_index.long()\n",
    "        combined = torch.cat([z[edge_index[0]], z[edge_index[1]]], dim=-1)\n",
    "        return torch.sigmoid(self.link_predictor(combined)).squeeze()\n",
    "    def predict_future(self, z_current):\n",
    "        return self.temporal_predictor(z_current)\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# â–¼â–¼â–¼ æ–°ã—ã„ãƒãƒƒãƒå¯¾å¿œã®æå¤±è¨ˆç®—é–¢æ•° â–¼â–¼â–¼\n",
    "# ==============================================================================\n",
    "def compute_batch_loss(model, batch_t, mu_t1_full, beta=0.01, pos_weight=10.0):\n",
    "    \"\"\"\n",
    "    ãƒŸãƒ‹ãƒãƒƒãƒãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ã£ã¦æå¤±ã‚’è¨ˆç®—ã™ã‚‹ã€‚\n",
    "    \"\"\"\n",
    "    # 1. ãƒŸãƒ‹ãƒãƒƒãƒå†…ã®ã‚µãƒ–ã‚°ãƒ©ãƒ•ã‚’ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰\n",
    "    node_indices_batch = batch_t.n_id\n",
    "    z_t, mu_t, logvar_t = model.encode(batch_t.x, batch_t.edge_index, node_indices_batch)\n",
    "\n",
    "    # 2. å†æ§‹æˆæå¤± (ãƒãƒƒãƒå†…ã®æ­£ä¾‹ãƒ»è² ä¾‹ãƒªãƒ³ã‚¯ã‚’ä½¿ç”¨)\n",
    "    pos_edge_index = batch_t.edge_label_index[:, batch_t.edge_label == 1]\n",
    "    neg_edge_index = batch_t.edge_label_index[:, batch_t.edge_label == 0]\n",
    "\n",
    "    pos_pred = model.decode(z_t, pos_edge_index)\n",
    "    neg_pred = model.decode(z_t, neg_edge_index)\n",
    "    \n",
    "    pos_loss = F.binary_cross_entropy(pos_pred, torch.ones_like(pos_pred), weight=torch.tensor(pos_weight, device=device))\n",
    "    neg_loss = F.binary_cross_entropy(neg_pred, torch.zeros_like(neg_pred))\n",
    "    recon_loss = pos_loss + neg_loss\n",
    "    \n",
    "    # 3. KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ (ãƒãƒƒãƒå†…ã®ãƒãƒ¼ãƒ‰ã®ã¿)\n",
    "    kl_loss = -0.5 * torch.mean(torch.sum(1 + logvar_t - mu_t.pow(2) - logvar_t.exp(), dim=1))\n",
    "    \n",
    "    # 4. æ½œåœ¨ç©ºé–“äºˆæ¸¬æå¤±\n",
    "    z_t1_pred = model.predict_future(z_t)\n",
    "    mu_t1_target = mu_t1_full[node_indices_batch] # äº‹å‰ã«è¨ˆç®—ã—ãŸt+1ã®å…¨ä½“Zã‹ã‚‰ãƒãƒƒãƒå†…ã®ãƒãƒ¼ãƒ‰ã«å¯¾å¿œã™ã‚‹éƒ¨åˆ†ã‚’æŠ½å‡º\n",
    "    latent_pred_loss = F.mse_loss(z_t1_pred, mu_t1_target)\n",
    "    \n",
    "    total_loss = recon_loss + beta * kl_loss + 0.5 * latent_pred_loss\n",
    "    \n",
    "    return total_loss, {'total_loss': total_loss.item()}\n",
    "\n",
    "# ==============================================================================\n",
    "# â–¼â–¼â–¼ ãƒãƒƒãƒå­¦ç¿’ã«å¯¾å¿œã—ãŸå­¦ç¿’ãƒ»è©•ä¾¡é–¢æ•° â–¼â–¼â–¼\n",
    "# ==============================================================================\n",
    "# train_model é–¢æ•°ã‚’ã¾ã‚‹ã”ã¨ã“ã¡ã‚‰ã«ç½®ãæ›ãˆã¦ãã ã•ã„\n",
    "def train_model(model, graph_dict, num_corps, model_name, num_epochs=40):\n",
    "    \"\"\"å˜ä¸€ã®ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ã¨æ¤œè¨¼ã‚’è¡Œã†ï¼ˆå¤ã„PyGãƒãƒ¼ã‚¸ãƒ§ãƒ³ï¼‹ãƒŸãƒ‹ãƒãƒƒãƒå¯¾å¿œç‰ˆï¼‰\"\"\"\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, patience=8, factor=0.5)\n",
    "    \n",
    "    years = sorted(graph_dict.keys())\n",
    "    train_years = years[:-2]\n",
    "    val_year = years[-2]\n",
    "    \n",
    "    history = {'train_loss': [], 'val_auc': []}\n",
    "    best_val_auc = 0\n",
    "    patience_counter = 0\n",
    "\n",
    "    print(f\"\\n--- Training {model_name} ---\")\n",
    "    \n",
    "    for epoch in tqdm(range(num_epochs), desc=f\"Training {model_name}\"):\n",
    "        model.train()\n",
    "        epoch_losses = []\n",
    "\n",
    "        # å¹´ã”ã¨ã®ãƒ«ãƒ¼ãƒ—\n",
    "        for i in range(len(train_years) - 1):\n",
    "            data_t = graph_dict[train_years[i]]\n",
    "            data_t1 = graph_dict[train_years[i+1]]\n",
    "            if data_t.edge_index.size(1) == 0: continue\n",
    "            \n",
    "            # â–¼â–¼â–¼ å¤ã„ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã®LinkLoaderã®æ­£ã—ã„ä½¿ã„æ–¹ã«ä¿®æ­£ â–¼â–¼â–¼\n",
    "            loader = LinkLoader(\n",
    "                data_t,\n",
    "                num_neighbors=[10, 5],\n",
    "                neg_sampling_ratio=1.0,\n",
    "                batch_size=2048,\n",
    "                shuffle=True,\n",
    "                num_workers=0, # num_workersã¯ç’°å¢ƒã«ã‚ˆã£ã¦èª¿æ•´\n",
    "            )\n",
    "\n",
    "            # äºˆæ¸¬æå¤±è¨ˆç®—ã®ãŸã‚ã«ã€t+1ã®ã‚°ãƒ©ãƒ•å…¨ä½“ã®æ½œåœ¨è¡¨ç¾ã‚’äº‹å‰ã«è¨ˆç®—\n",
    "            with torch.no_grad():\n",
    "                all_node_indices = torch.arange(data_t1.num_nodes, device=device)\n",
    "                _, mu_t1_full, _ = model.encode(data_t1.x.to(device), data_t1.edge_index.to(device), all_node_indices)\n",
    "\n",
    "            # â–¼â–¼â–¼ ãƒŸãƒ‹ãƒãƒƒãƒã”ã¨ã®ãƒ«ãƒ¼ãƒ—ï¼ˆãƒ­ã‚¸ãƒƒã‚¯ã‚’ä¿®æ­£ï¼‰ â–¼â–¼â–¼\n",
    "            for batch_t in loader:\n",
    "                batch_t = batch_t.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # ãƒãƒƒãƒãƒ‡ãƒ¼ã‚¿ã§æå¤±è¨ˆç®—ã¨æ›´æ–°\n",
    "                # compute_batch_loss ã¯å‰å›ã®ä¿®æ­£ã§ä½œæˆã—ãŸã‚‚ã®ã‚’ãã®ã¾ã¾ä½¿ãˆã¾ã™\n",
    "                loss, loss_dict = compute_batch_loss(model, batch_t, mu_t1_full)\n",
    "                \n",
    "                if not torch.isnan(loss):\n",
    "                    loss.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                    optimizer.step()\n",
    "                    epoch_losses.append(loss_dict['total_loss'])\n",
    "        \n",
    "        avg_loss = np.mean(epoch_losses) if epoch_losses else 0\n",
    "        history['train_loss'].append(avg_loss)\n",
    "        \n",
    "        # è©•ä¾¡\n",
    "        val_result = evaluate_model(model, graph_dict[val_year], num_corps)\n",
    "        val_auc = val_result['auc'] if val_result else 0\n",
    "        history['val_auc'].append(val_auc)\n",
    "        \n",
    "        scheduler.step(avg_loss)\n",
    "\n",
    "        if val_auc > best_val_auc:\n",
    "            best_val_auc = val_auc\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), f'best_{model_name.lower().replace(\" \", \"_\").replace(\"+\", \"_\")}_model.pth')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        if patience_counter >= 15:\n",
    "            print(f\"Early stopping at epoch {epoch}\")\n",
    "            break\n",
    "        \n",
    "        gc.collect() # ãƒ¡ãƒ¢ãƒªè§£æ”¾\n",
    "            \n",
    "    print(f\"Finished training. Best Validation AUC: {best_val_auc:.4f}\")\n",
    "    return model, history, best_val_auc\n",
    "\n",
    "def evaluate_model(model, data, num_corps):\n",
    "    \"\"\"æŒ‡å®šã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã§ãƒ¢ãƒ‡ãƒ«ã‚’è©•ä¾¡ã—ã€AUCã¨APã‚’è¿”ã™ (å¤‰æ›´ãªã—)\"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        node_indices = torch.arange(data.num_nodes, device=device)\n",
    "        # è©•ä¾¡æ™‚ã¯å…¨ãƒãƒ¼ãƒ‰ã®æœ€çµ‚çš„ãªã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ã‚’å–å¾—\n",
    "        z, _, _ = model.encode(data.x.to(device), data.edge_index.to(device), node_indices)\n",
    "        \n",
    "        pos_edge_index = data.edge_index.to(device)\n",
    "        if pos_edge_index.size(1) == 0: return None\n",
    "        pos_scores = model.decode(z, pos_edge_index)\n",
    "        \n",
    "        # è² ä¾‹ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°\n",
    "        neg_edge_index = torch.randint(0, data.num_nodes, pos_edge_index.size(), dtype=torch.long, device=device)\n",
    "        neg_scores = model.decode(z, neg_edge_index)\n",
    "\n",
    "        y_true = torch.cat([torch.ones_like(pos_scores), torch.zeros_like(neg_scores)]).cpu().numpy()\n",
    "        y_scores = torch.cat([pos_scores, neg_scores]).cpu().numpy()\n",
    "        \n",
    "        return {\n",
    "            'auc': roc_auc_score(y_true, y_scores),\n",
    "            'ap': average_precision_score(y_true, y_scores)\n",
    "        }\n",
    "\n",
    "# ==============================================================================\n",
    "# å®Ÿé¨“å®Ÿè¡Œã¨çµæœåˆ†æ (å¤‰æ›´ãªã—)\n",
    "# ==============================================================================\n",
    "def plot_comparison_results(results, histories):\n",
    "    \"\"\"å®Ÿé¨“çµæœã‚’å¯è¦–åŒ–ã™ã‚‹\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    model_names = list(results.keys())\n",
    "    colors = ['#FF6B6B', '#4ECDC4', '#45B7D1']\n",
    "    test_aucs = [results[name]['test_auc'] for name in model_names]\n",
    "    axes[0, 0].bar(model_names, test_aucs, color=colors, alpha=0.8)\n",
    "    axes[0, 0].set_title('Test AUC Performance')\n",
    "    axes[0, 0].set_ylabel('AUC Score')\n",
    "    if all(v > 0 for v in test_aucs): axes[0, 0].set_ylim(bottom=min(test_aucs) * 0.95)\n",
    "    for i, v in enumerate(test_aucs): axes[0, 0].text(i, v, f'{v:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "    test_aps = [results[name]['test_ap'] for name in model_names]\n",
    "    axes[0, 1].bar(model_names, test_aps, color=colors, alpha=0.8)\n",
    "    axes[0, 1].set_title('Test Average Precision (AP)')\n",
    "    axes[0, 1].set_ylabel('AP Score')\n",
    "    if all(v > 0 for v in test_aps): axes[0, 1].set_ylim(bottom=min(test_aps) * 0.95)\n",
    "    for i, v in enumerate(test_aps): axes[0, 1].text(i, v, f'{v:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "    training_times = [results[name]['training_time'] for name in model_names]\n",
    "    axes[1, 0].bar(model_names, training_times, color=colors, alpha=0.8)\n",
    "    axes[1, 0].set_title('Training Time')\n",
    "    axes[1, 0].set_ylabel('Time (seconds)')\n",
    "    for i, v in enumerate(training_times): axes[1, 0].text(i, v, f'{v:.1f}s', ha='center', va='bottom', fontweight='bold')\n",
    "    for name, color in zip(model_names, colors):\n",
    "        axes[1, 1].plot(histories[name]['val_auc'], label=name, color=color, linewidth=2, marker='o', markersize=4)\n",
    "    axes[1, 1].set_title('Validation AUC during Training')\n",
    "    axes[1, 1].set_xlabel('Epoch')\n",
    "    axes[1, 1].set_ylabel('Validation AUC')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.4)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def main():\n",
    "    \"\"\"å®Ÿé¨“å…¨ä½“ã‚’ç®¡ç†ãƒ»å®Ÿè¡Œã™ã‚‹ãƒ¡ã‚¤ãƒ³é–¢æ•°\"\"\"\n",
    "    print(\"1. Loading and preprocessing data...\")\n",
    "    cleaned_df, feature_cols = preprocess_data()\n",
    "    \n",
    "    if cleaned_df.empty:\n",
    "        print(\"Error: No data loaded. Please check file paths and data availability.\")\n",
    "        return {}, {}\n",
    "    \n",
    "    graph_dict, corps, patents, num_nodes = build_global_graphs(cleaned_df, feature_cols)\n",
    "    print(f\"Data loaded: {len(corps)} corporations, {len(patents)} patents, {len(graph_dict)} yearly graphs.\")\n",
    "\n",
    "    if not graph_dict:\n",
    "        print(\"Error: No graphs built. Please check data quality.\")\n",
    "        return {}, {}\n",
    "\n",
    "    years = sorted(graph_dict.keys())\n",
    "    test_year = years[-1]\n",
    "    num_corps = len(corps)\n",
    "    \n",
    "    # ç‰¹å¾´é‡ã®æ¬¡å…ƒã‚’å‹•çš„ã«æ±ºå®š\n",
    "    input_dim = len(feature_cols) if feature_cols else 128\n",
    "    print(f\"Using input dimension: {input_dim}\")\n",
    "\n",
    "    models_to_compare = {\n",
    "        'VGAE+ODE': UnifiedVGAE(num_nodes, num_corps, input_dim=input_dim, predictor_type='ode', hidden_dim=32, latent_dim=16),\n",
    "        'VGAE+RNN': UnifiedVGAE(num_nodes, num_corps, input_dim=input_dim, predictor_type='rnn', hidden_dim=32, latent_dim=16),\n",
    "        'VGAE+LSTM': UnifiedVGAE(num_nodes, num_corps, input_dim=input_dim, predictor_type='lstm', hidden_dim=32, latent_dim=16)\n",
    "    }\n",
    "\n",
    "    results = {}\n",
    "    histories = {}\n",
    "    \n",
    "    print(\"\\n2. Starting model training and evaluation...\")\n",
    "    for name, model in models_to_compare.items():\n",
    "        start_time = time.time()\n",
    "        trained_model, history, best_val_auc = train_model(model, graph_dict, num_corps, name, num_epochs=50)\n",
    "        training_time = time.time() - start_time\n",
    "        histories[name] = history\n",
    "        try:\n",
    "            model_path = f'best_{name.lower().replace(\" \", \"_\").replace(\"+\", \"_\")}_model.pth'\n",
    "            trained_model.load_state_dict(torch.load(model_path))\n",
    "            test_result = evaluate_model(trained_model, graph_dict[test_year], num_corps)\n",
    "        except Exception as e:\n",
    "            print(f\"Could not load or evaluate model {name}: {e}\")\n",
    "            test_result = None\n",
    "        \n",
    "        results[name] = {\n",
    "            'best_val_auc': best_val_auc,\n",
    "            'test_auc': test_result['auc'] if test_result else 0,\n",
    "            'test_ap': test_result['ap'] if test_result else 0,\n",
    "            'training_time': training_time,\n",
    "            'num_params': sum(p.numel() for p in model.parameters())\n",
    "        }\n",
    "\n",
    "    print(\"\\n3. Final Results Summary:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"{'Model':<15} | {'Test AUC':<12} | {'Test AP':<12} | {'Val AUC (Best)':<18} | {'Training Time':<15} | {'Parameters':<12}\")\n",
    "    print(\"-\" * 80)\n",
    "    for name, r in results.items():\n",
    "        print(f\"{name:<15} | {r['test_auc']:<12.4f} | {r['test_ap']:<12.4f} | {r['best_val_auc']:<18.4f} | {r['training_time']:<15.1f}s | {r['num_params']:,}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    print(\"\\n4. Plotting results...\")\n",
    "    plot_comparison_results(results, histories)\n",
    "\n",
    "    best_model_name = max(results, key=lambda name: results[name]['test_auc'])\n",
    "    print(f\"\\nExperiment Conclusion: The best performing model is '{best_model_name}' with a Test AUC of {results[best_model_name]['test_auc']:.4f}.\")\n",
    "    \n",
    "    return results, histories\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    results, histories = main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264778fc",
   "metadata": {},
   "source": [
    "ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯ã€ç‰¹è¨±ãƒ‡ãƒ¼ã‚¿ã‚’ç”¨ã„ã¦å‹•çš„ãƒªãƒ³ã‚¯äºˆæ¸¬ã‚¿ã‚¹ã‚¯ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚VGAEï¼ˆVariational Graph Autoencoderï¼‰ã‚’å…±é€šã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã¨ã—ã¦ä½¿ç”¨ã—ã€æ™‚é–“çš„ãªé–¢ä¿‚æ€§ã‚’ãƒ¢ãƒ‡ãƒ«åŒ–ã™ã‚‹éƒ¨åˆ†ï¼ˆæ™‚ç³»åˆ—äºˆæ¸¬å™¨ï¼‰ã‚’NeuralODEã€RNNã€LSTMã®3ç¨®é¡ã§å·®ã—æ›¿ãˆã¦æ€§èƒ½ã‚’æ¯”è¼ƒã™ã‚‹å®Ÿé¨“ã‚’è¡Œã„ã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ã©ã®æ™‚ç³»åˆ—ãƒ¢ãƒ‡ãƒªãƒ³ã‚°æ‰‹æ³•ãŒæŠ€è¡“ãƒˆãƒ¬ãƒ³ãƒ‰äºˆæ¸¬ã«æœ€ã‚‚é©ã—ã¦ã„ã‚‹ã‹ã‚’å…¬å¹³ã«è©•ä¾¡ã™ã‚‹ã“ã¨ã‚’ç›®çš„ã¨ã—ã¦ã„ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4429c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "1. Loading and preprocessing data...\n",
      "Data loaded: 4777 corporations, 49262 patents, 33 yearly graphs.\n",
      "\n",
      "2. Starting model training and evaluation...\n",
      "\n",
      "--- Training VGAE+ODE ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training VGAE+ODE:   0%|          | 0/50 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "LinkLoader.__init__() missing 1 required positional argument: 'link_sampler'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 420\u001b[0m\n\u001b[1;32m    416\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results, histories\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 420\u001b[0m     results, histories \u001b[38;5;241m=\u001b[39m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 383\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, model \u001b[38;5;129;01min\u001b[39;00m models_to_compare\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    382\u001b[0m     start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 383\u001b[0m     trained_model, history, best_val_auc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgraph_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_corps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    384\u001b[0m     training_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[1;32m    385\u001b[0m     histories[name] \u001b[38;5;241m=\u001b[39m history\n",
      "Cell \u001b[0;32mIn[7], line 246\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, graph_dict, num_corps, model_name, num_epochs)\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m data_t\u001b[38;5;241m.\u001b[39medge_index\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m: \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;66;03m# â–¼â–¼â–¼ å¤ã„ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã®LinkLoaderã®æ­£ã—ã„ä½¿ã„æ–¹ã«ä¿®æ­£ â–¼â–¼â–¼\u001b[39;00m\n\u001b[0;32m--> 246\u001b[0m loader \u001b[38;5;241m=\u001b[39m \u001b[43mLinkLoader\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_t\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_neighbors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m    \u001b[49m\u001b[43mneg_sampling_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2048\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# num_workersã¯ç’°å¢ƒã«ã‚ˆã£ã¦èª¿æ•´\u001b[39;49;00m\n\u001b[1;32m    253\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;66;03m# äºˆæ¸¬æå¤±è¨ˆç®—ã®ãŸã‚ã«ã€t+1ã®ã‚°ãƒ©ãƒ•å…¨ä½“ã®æ½œåœ¨è¡¨ç¾ã‚’äº‹å‰ã«è¨ˆç®—\u001b[39;00m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "\u001b[0;31mTypeError\u001b[0m: LinkLoader.__init__() missing 1 required positional argument: 'link_sampler'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "from torchdiffeq import odeint\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import japanize_matplotlib\n",
    "import gc\n",
    "import os\n",
    "\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# å®šæ•°å®šç¾©\n",
    "# ==============================================================================\n",
    "# ç¾åœ¨ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«åŸºã¥ã„ã¦ãƒ‘ã‚¹ã‚’è¨­å®š\n",
    "current_dir = os.getcwd()\n",
    "if 'work' in current_dir:\n",
    "    base_path = current_dir\n",
    "else:\n",
    "    base_path = os.path.join(current_dir, 'work')\n",
    "\n",
    "KG_DATASET_PATH = os.path.join(base_path, \"dataset\", \"kg_dataset.csv\")\n",
    "PATENT_DATA_PATH = os.path.join(base_path, \"processed_data\", \"kumagai_patentdata.csv\")\n",
    "\n",
    "print(f\"KG Dataset path: {KG_DATASET_PATH}\")\n",
    "print(f\"Patent data path: {PATENT_DATA_PATH}\")\n",
    "print(f\"Files exist - KG: {os.path.exists(KG_DATASET_PATH)}, Patent: {os.path.exists(PATENT_DATA_PATH)}\")\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç† (å¤‰æ›´ãªã—)\n",
    "# ==============================================================================\n",
    "def preprocess_data(kg_path=KG_DATASET_PATH, patent_path=PATENT_DATA_PATH):\n",
    "    \"\"\"\n",
    "    2ã¤ã®CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã€ãƒãƒ¼ã‚¸ã—ã¦å‰å‡¦ç†ã‚’è¡Œã†ã€‚\n",
    "    ç‰¹è¨±ã¨ä¼æ¥­ã®æ™‚ç³»åˆ—ã‚°ãƒ©ãƒ•æ§‹ç¯‰ã«å¿…è¦ãªãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’è¿”ã™ã€‚\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"Loading KG dataset from: {kg_path}\")\n",
    "        kg_df = pd.read_csv(kg_path)\n",
    "        print(f\"KG dataset shape: {kg_df.shape}\")\n",
    "        \n",
    "        print(f\"Loading patent data from: {patent_path}\")\n",
    "        kumagai_df = pd.read_csv(patent_path)\n",
    "        print(f\"Patent data shape: {kumagai_df.shape}\")\n",
    "        \n",
    "        merged_df = pd.merge(kg_df, kumagai_df, on=\"patent_number\", how=\"left\")\n",
    "        print(f\"Merged data shape: {merged_df.shape}\")\n",
    "        \n",
    "        cleaned_df = merged_df.dropna(subset=[\"corporation\", \"patent_number\"], how=\"any\")\n",
    "        print(f\"Cleaned data shape: {cleaned_df.shape}\")\n",
    "        \n",
    "        feature_cols = [f\"pca_text_dim_{i}\" for i in range(64)] + [f\"node_dim_{i}\" for i in range(64)]\n",
    "        # å­˜åœ¨ã™ã‚‹ã‚«ãƒ©ãƒ ã®ã¿ã‚’å‡¦ç†\n",
    "        existing_feature_cols = [col for col in feature_cols if col in cleaned_df.columns]\n",
    "        print(f\"Found {len(existing_feature_cols)} feature columns out of {len(feature_cols)} expected\")\n",
    "        \n",
    "        if existing_feature_cols:\n",
    "            cleaned_df[existing_feature_cols] = cleaned_df[existing_feature_cols].apply(pd.to_numeric, errors='coerce').replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "            cleaned_df[existing_feature_cols] = cleaned_df[existing_feature_cols].astype(np.float32)\n",
    "        \n",
    "        def safe_literal_eval(s):\n",
    "            try: \n",
    "                return ast.literal_eval(s)\n",
    "            except (ValueError, SyntaxError): \n",
    "                return []\n",
    "        \n",
    "        cleaned_df[\"corporation\"] = cleaned_df[\"corporation\"].apply(safe_literal_eval)\n",
    "        cleaned_df['year_month'] = pd.to_datetime(cleaned_df['year_month'], format='%Y-%m')\n",
    "        \n",
    "        return cleaned_df, existing_feature_cols\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in preprocess_data: {e}\")\n",
    "        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ã®ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’è¿”ã™\n",
    "        return pd.DataFrame(), []\n",
    "\n",
    "def build_global_graphs(df, feature_cols):\n",
    "    \"\"\"\n",
    "    ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‹ã‚‰å¹´ã”ã¨ã®ã‚°ãƒ©ãƒ•ãƒ‡ãƒ¼ã‚¿ã‚’æ§‹ç¯‰ã™ã‚‹ã€‚\n",
    "    å…¨ã¦ã®ãƒãƒ¼ãƒ‰ï¼ˆä¼æ¥­ãƒ»ç‰¹è¨±ï¼‰ã‚’å«ã‚€ã‚°ãƒ­ãƒ¼ãƒãƒ«ãªã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ç©ºé–“ã§ã‚°ãƒ©ãƒ•ã‚’ä½œæˆã™ã‚‹ã€‚\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        print(\"Warning: Empty dataframe provided to build_global_graphs\")\n",
    "        return {}, [], [], 0\n",
    "    \n",
    "    year_groups = df.groupby(df['year_month'].dt.year)\n",
    "    all_corporations = sorted(list(set(c for corps in df['corporation'] for c in corps)))\n",
    "    all_patents = sorted(list(df['patent_number'].unique()))\n",
    "    corp_to_idx = {corp: i for i, corp in enumerate(all_corporations)}\n",
    "    patent_to_idx = {patent: i + len(all_corporations) for i, patent in enumerate(all_patents)}\n",
    "    num_total_nodes = len(all_corporations) + len(all_patents)\n",
    "\n",
    "    print(f\"Building graphs: {len(all_corporations)} corporations, {len(all_patents)} patents, {num_total_nodes} total nodes\")\n",
    "\n",
    "    graph_dict = {}\n",
    "    for year, group in year_groups:\n",
    "        edges = []\n",
    "        for _, row in group.iterrows():\n",
    "            for corp in row['corporation']:\n",
    "                if corp in corp_to_idx and row['patent_number'] in patent_to_idx:\n",
    "                    edges.append([corp_to_idx[corp], patent_to_idx[row['patent_number']]])\n",
    "        if not edges: \n",
    "            print(f\"No edges found for year {year}, skipping\")\n",
    "            continue\n",
    "\n",
    "        # ç‰¹å¾´é‡ã®æ¬¡å…ƒã‚’å‹•çš„ã«æ±ºå®š\n",
    "        feature_dim = len(feature_cols) if feature_cols else 128\n",
    "        x = torch.randn(num_total_nodes, feature_dim) # ç‰¹å¾´é‡ã¯ãƒãƒ¼ãƒ‰å…¨ä½“ã§å…±æœ‰\n",
    "        \n",
    "        for _, row in group.iterrows():\n",
    "            if row['patent_number'] in patent_to_idx:\n",
    "                patent_idx = patent_to_idx[row['patent_number']]\n",
    "                if feature_cols:\n",
    "                    # åˆ©ç”¨å¯èƒ½ãªç‰¹å¾´é‡ã‚’ä½¿ç”¨\n",
    "                    available_features = [col for col in feature_cols if col in row.index]\n",
    "                    if available_features:\n",
    "                        feat_values = row[available_features].values.astype(np.float32)\n",
    "                        # ç‰¹å¾´é‡ã®æ¬¡å…ƒã‚’åˆã‚ã›ã‚‹\n",
    "                        if len(feat_values) < feature_dim:\n",
    "                            feat_values = np.pad(feat_values, (0, feature_dim - len(feat_values)), 'constant')\n",
    "                        elif len(feat_values) > feature_dim:\n",
    "                            feat_values = feat_values[:feature_dim]\n",
    "                        x[patent_idx] = torch.tensor(feat_values, dtype=torch.float32)\n",
    "\n",
    "        edge_index = torch.tensor(edges, dtype=torch.long).t()\n",
    "        graph_dict[year] = Data(x=x, edge_index=edge_index, num_nodes=num_total_nodes, year=year)\n",
    "        print(f\"Year {year}: {edge_index.size(1)} edges\")\n",
    "\n",
    "    return graph_dict, all_corporations, all_patents, num_total_nodes\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# ãƒ¢ãƒ‡ãƒ«å®šç¾© (å¤‰æ›´ãªã—)\n",
    "# ==============================================================================\n",
    "class SharedVGAEEncoder(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels * 2)\n",
    "        self.conv2 = GCNConv(hidden_channels * 2, hidden_channels)\n",
    "        self.conv3 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.conv_mu = GCNConv(hidden_channels, out_channels)\n",
    "        self.conv_logvar = GCNConv(hidden_channels, out_channels)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.batch_norm1 = nn.BatchNorm1d(hidden_channels * 2)\n",
    "        self.batch_norm2 = nn.BatchNorm1d(hidden_channels)\n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.relu(self.batch_norm1(self.conv1(x, edge_index)))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.batch_norm2(self.conv2(x, edge_index)))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.conv3(x, edge_index))\n",
    "        return self.conv_mu(x, edge_index), self.conv_logvar(x, edge_index)\n",
    "\n",
    "class ODEFunc(nn.Module):\n",
    "    def __init__(self, latent_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(nn.Linear(latent_dim, hidden_dim), nn.Tanh(), nn.Linear(hidden_dim, hidden_dim), nn.Tanh(), nn.Linear(hidden_dim, latent_dim))\n",
    "    def forward(self, t, z): return self.net(z)\n",
    "\n",
    "class NeuralODEPredictor(nn.Module):\n",
    "    def __init__(self, latent_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.ode_func = ODEFunc(latent_dim, hidden_dim)\n",
    "    def forward(self, z_current, delta_t=1.0):\n",
    "        t_span = torch.tensor([0., delta_t], device=z_current.device)\n",
    "        return odeint(self.ode_func, z_current, t_span, method='dopri5')[-1]\n",
    "\n",
    "class RNNPredictor(nn.Module):\n",
    "    def __init__(self, latent_dim, hidden_dim, num_layers=2):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.RNN(latent_dim, hidden_dim, num_layers, batch_first=True, dropout=0.2)\n",
    "        self.output_layer = nn.Linear(hidden_dim, latent_dim)\n",
    "    def forward(self, z_sequence):\n",
    "        if z_sequence.dim() == 2: z_sequence = z_sequence.unsqueeze(1)\n",
    "        rnn_out, _ = self.rnn(z_sequence)\n",
    "        return self.output_layer(rnn_out[:, -1, :])\n",
    "\n",
    "class LSTMPredictor(nn.Module):\n",
    "    def __init__(self, latent_dim, hidden_dim, num_layers=2):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(latent_dim, hidden_dim, num_layers, batch_first=True, dropout=0.2)\n",
    "        self.output_layer = nn.Linear(hidden_dim, latent_dim)\n",
    "    def forward(self, z_sequence):\n",
    "        if z_sequence.dim() == 2: z_sequence = z_sequence.unsqueeze(1)\n",
    "        lstm_out, _ = self.lstm(z_sequence)\n",
    "        return self.output_layer(lstm_out[:, -1, :])\n",
    "\n",
    "class UnifiedVGAE(nn.Module):\n",
    "    def __init__(self, num_nodes, num_corps, input_dim=128, hidden_dim=64, latent_dim=16, predictor_type='ode'):\n",
    "        super().__init__()\n",
    "        self.num_nodes = num_nodes\n",
    "        self.num_corps = num_corps\n",
    "        self.input_dim = input_dim\n",
    "        self.corp_embeddings = nn.Embedding(num_corps, input_dim)\n",
    "        nn.init.xavier_uniform_(self.corp_embeddings.weight)\n",
    "        self.encoder = SharedVGAEEncoder(input_dim, hidden_dim, latent_dim)\n",
    "        if predictor_type == 'ode': self.temporal_predictor = NeuralODEPredictor(latent_dim, hidden_dim)\n",
    "        elif predictor_type == 'rnn': self.temporal_predictor = RNNPredictor(latent_dim, hidden_dim)\n",
    "        elif predictor_type == 'lstm': self.temporal_predictor = LSTMPredictor(latent_dim, hidden_dim)\n",
    "        else: raise ValueError(f\"Unknown predictor type: {predictor_type}\")\n",
    "        self.link_predictor = nn.Sequential(nn.Linear(latent_dim * 2, hidden_dim), nn.ReLU(), nn.Dropout(0.3), nn.Linear(hidden_dim, hidden_dim // 2), nn.ReLU(), nn.Linear(hidden_dim // 2, 1))\n",
    "    def get_node_features(self, x, node_indices):\n",
    "        features = x.clone()\n",
    "        corp_mask = node_indices < self.num_corps\n",
    "        if corp_mask.any():\n",
    "            corp_indices = node_indices[corp_mask]\n",
    "            # ç‰¹å¾´é‡ã®æ¬¡å…ƒã‚’åˆã‚ã›ã‚‹\n",
    "            corp_embeddings = self.corp_embeddings(corp_indices)\n",
    "            if corp_embeddings.size(1) != features.size(1):\n",
    "                # æ¬¡å…ƒãŒåˆã‚ãªã„å ´åˆã¯èª¿æ•´\n",
    "                if corp_embeddings.size(1) < features.size(1):\n",
    "                    padding = torch.zeros(corp_embeddings.size(0), features.size(1) - corp_embeddings.size(1), device=corp_embeddings.device)\n",
    "                    corp_embeddings = torch.cat([corp_embeddings, padding], dim=1)\n",
    "                else:\n",
    "                    corp_embeddings = corp_embeddings[:, :features.size(1)]\n",
    "            features[corp_mask] = corp_embeddings\n",
    "        return features\n",
    "    def encode(self, x, edge_index, node_indices):\n",
    "        x_featured = self.get_node_features(x, node_indices)\n",
    "        mu, logvar = self.encoder(x_featured, edge_index.long())\n",
    "        if self.training:\n",
    "            std = torch.exp(0.5 * logvar)\n",
    "            eps = torch.randn_like(std)\n",
    "            return mu + eps * std, mu, logvar\n",
    "        return mu, mu, logvar\n",
    "    def decode(self, z, edge_index):\n",
    "        edge_index = edge_index.long()\n",
    "        combined = torch.cat([z[edge_index[0]], z[edge_index[1]]], dim=-1)\n",
    "        return torch.sigmoid(self.link_predictor(combined)).squeeze()\n",
    "    def predict_future(self, z_current):\n",
    "        return self.temporal_predictor(z_current)\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# â–¼â–¼â–¼ æ–°ã—ã„ãƒãƒƒãƒå¯¾å¿œã®æå¤±è¨ˆç®—é–¢æ•° â–¼â–¼â–¼\n",
    "# ==============================================================================\n",
    "def compute_batch_loss(model, batch_t, mu_t1_full, beta=0.01, pos_weight=10.0):\n",
    "    \"\"\"\n",
    "    ãƒŸãƒ‹ãƒãƒƒãƒãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ã£ã¦æå¤±ã‚’è¨ˆç®—ã™ã‚‹ã€‚\n",
    "    \"\"\"\n",
    "    # 1. ãƒŸãƒ‹ãƒãƒƒãƒå†…ã®ã‚µãƒ–ã‚°ãƒ©ãƒ•ã‚’ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰\n",
    "    node_indices_batch = batch_t.n_id\n",
    "    z_t, mu_t, logvar_t = model.encode(batch_t.x, batch_t.edge_index, node_indices_batch)\n",
    "\n",
    "    # 2. å†æ§‹æˆæå¤± (ãƒãƒƒãƒå†…ã®æ­£ä¾‹ãƒ»è² ä¾‹ãƒªãƒ³ã‚¯ã‚’ä½¿ç”¨)\n",
    "    pos_edge_index = batch_t.edge_label_index[:, batch_t.edge_label == 1]\n",
    "    neg_edge_index = batch_t.edge_label_index[:, batch_t.edge_label == 0]\n",
    "\n",
    "    pos_pred = model.decode(z_t, pos_edge_index)\n",
    "    neg_pred = model.decode(z_t, neg_edge_index)\n",
    "    \n",
    "    pos_loss = F.binary_cross_entropy(pos_pred, torch.ones_like(pos_pred), weight=torch.tensor(pos_weight, device=device))\n",
    "    neg_loss = F.binary_cross_entropy(neg_pred, torch.zeros_like(neg_pred))\n",
    "    recon_loss = pos_loss + neg_loss\n",
    "    \n",
    "    # 3. KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ (ãƒãƒƒãƒå†…ã®ãƒãƒ¼ãƒ‰ã®ã¿)\n",
    "    kl_loss = -0.5 * torch.mean(torch.sum(1 + logvar_t - mu_t.pow(2) - logvar_t.exp(), dim=1))\n",
    "    \n",
    "    # 4. æ½œåœ¨ç©ºé–“äºˆæ¸¬æå¤±\n",
    "    z_t1_pred = model.predict_future(z_t)\n",
    "    mu_t1_target = mu_t1_full[node_indices_batch] # äº‹å‰ã«è¨ˆç®—ã—ãŸt+1ã®å…¨ä½“Zã‹ã‚‰ãƒãƒƒãƒå†…ã®ãƒãƒ¼ãƒ‰ã«å¯¾å¿œã™ã‚‹éƒ¨åˆ†ã‚’æŠ½å‡º\n",
    "    latent_pred_loss = F.mse_loss(z_t1_pred, mu_t1_target)\n",
    "    \n",
    "    total_loss = recon_loss + beta * kl_loss + 0.5 * latent_pred_loss\n",
    "    \n",
    "    return total_loss, {'total_loss': total_loss.item()}\n",
    "\n",
    "# ==============================================================================\n",
    "# â–¼â–¼â–¼ ãƒãƒƒãƒå­¦ç¿’ã«å¯¾å¿œã—ãŸå­¦ç¿’ãƒ»è©•ä¾¡é–¢æ•° â–¼â–¼â–¼\n",
    "# ==============================================================================\n",
    "# train_model é–¢æ•°ã‚’ã¾ã‚‹ã”ã¨ã“ã¡ã‚‰ã«ç½®ãæ›ãˆã¦ãã ã•ã„\n",
    "def train_model(model, graph_dict, num_corps, model_name, num_epochs=40):\n",
    "    \"\"\"å˜ä¸€ã®ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ã¨æ¤œè¨¼ã‚’è¡Œã†ï¼ˆç°¡æ˜“ç‰ˆï¼‰\"\"\"\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, patience=8, factor=0.5)\n",
    "    \n",
    "    years = sorted(graph_dict.keys())\n",
    "    train_years = years[:-2]\n",
    "    val_year = years[-2]\n",
    "    \n",
    "    history = {'train_loss': [], 'val_auc': []}\n",
    "    best_val_auc = 0\n",
    "    patience_counter = 0\n",
    "\n",
    "    print(f\"\\n--- Training {model_name} ---\")\n",
    "    \n",
    "    for epoch in tqdm(range(num_epochs), desc=f\"Training {model_name}\"):\n",
    "        model.train()\n",
    "        epoch_losses = []\n",
    "\n",
    "        # å¹´ã”ã¨ã®ãƒ«ãƒ¼ãƒ—ï¼ˆç°¡æ˜“ç‰ˆï¼šãƒãƒƒãƒå‡¦ç†ãªã—ï¼‰\n",
    "        for i in range(len(train_years) - 1):\n",
    "            data_t = graph_dict[train_years[i]]\n",
    "            data_t1 = graph_dict[train_years[i+1]]\n",
    "            if data_t.edge_index.size(1) == 0: continue\n",
    "            \n",
    "            # ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ‡ãƒã‚¤ã‚¹ã«ç§»å‹•\n",
    "            data_t = data_t.to(device)\n",
    "            data_t1 = data_t1.to(device)\n",
    "            \n",
    "            # å…¨ãƒãƒ¼ãƒ‰ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹\n",
    "            all_node_indices = torch.arange(data_t.num_nodes, device=device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # ç¾åœ¨ã®æ™‚åˆ»ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰\n",
    "            z_t, mu_t, logvar_t = model.encode(data_t.x, data_t.edge_index, all_node_indices)\n",
    "            \n",
    "            # å†æ§‹æˆæå¤±\n",
    "            pos_edge_index = data_t.edge_index\n",
    "            pos_pred = model.decode(z_t, pos_edge_index)\n",
    "            pos_loss = F.binary_cross_entropy(pos_pred, torch.ones_like(pos_pred))\n",
    "            \n",
    "            # è² ä¾‹ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°\n",
    "            neg_edge_index = torch.randint(0, data_t.num_nodes, pos_edge_index.size(), dtype=torch.long, device=device)\n",
    "            neg_pred = model.decode(z_t, neg_edge_index)\n",
    "            neg_loss = F.binary_cross_entropy(neg_pred, torch.zeros_like(neg_pred))\n",
    "            \n",
    "            recon_loss = pos_loss + neg_loss\n",
    "            \n",
    "            # KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹\n",
    "            kl_loss = -0.5 * torch.mean(torch.sum(1 + logvar_t - mu_t.pow(2) - logvar_t.exp(), dim=1))\n",
    "            \n",
    "            # äºˆæ¸¬æå¤±\n",
    "            z_t1_pred = model.predict_future(z_t)\n",
    "            _, mu_t1, _ = model.encode(data_t1.x, data_t1.edge_index, all_node_indices)\n",
    "            latent_pred_loss = F.mse_loss(z_t1_pred, mu_t1)\n",
    "            \n",
    "            # ç·æå¤±\n",
    "            total_loss = recon_loss + 0.01 * kl_loss + 0.5 * latent_pred_loss\n",
    "            \n",
    "            if not torch.isnan(total_loss):\n",
    "                total_loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "                epoch_losses.append(total_loss.item())\n",
    "        \n",
    "        avg_loss = np.mean(epoch_losses) if epoch_losses else 0\n",
    "        history['train_loss'].append(avg_loss)\n",
    "        \n",
    "        # è©•ä¾¡\n",
    "        val_result = evaluate_model(model, graph_dict[val_year], num_corps)\n",
    "        val_auc = val_result['auc'] if val_result else 0\n",
    "        history['val_auc'].append(val_auc)\n",
    "        \n",
    "        scheduler.step(avg_loss)\n",
    "\n",
    "        if val_auc > best_val_auc:\n",
    "            best_val_auc = val_auc\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), f'best_{model_name.lower().replace(\" \", \"_\").replace(\"+\", \"_\")}_model.pth')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        if patience_counter >= 15:\n",
    "            print(f\"Early stopping at epoch {epoch}\")\n",
    "            break\n",
    "        \n",
    "        gc.collect() # ãƒ¡ãƒ¢ãƒªè§£æ”¾\n",
    "            \n",
    "    print(f\"Finished training. Best Validation AUC: {best_val_auc:.4f}\")\n",
    "    return model, history, best_val_auc\n",
    "\n",
    "def evaluate_model(model, data, num_corps):\n",
    "    \"\"\"æŒ‡å®šã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã§ãƒ¢ãƒ‡ãƒ«ã‚’è©•ä¾¡ã—ã€AUCã¨APã‚’è¿”ã™ (å¤‰æ›´ãªã—)\"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        node_indices = torch.arange(data.num_nodes, device=device)\n",
    "        # è©•ä¾¡æ™‚ã¯å…¨ãƒãƒ¼ãƒ‰ã®æœ€çµ‚çš„ãªã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ã‚’å–å¾—\n",
    "        z, _, _ = model.encode(data.x.to(device), data.edge_index.to(device), node_indices)\n",
    "        \n",
    "        pos_edge_index = data.edge_index.to(device)\n",
    "        if pos_edge_index.size(1) == 0: return None\n",
    "        pos_scores = model.decode(z, pos_edge_index)\n",
    "        \n",
    "        # è² ä¾‹ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°\n",
    "        neg_edge_index = torch.randint(0, data.num_nodes, pos_edge_index.size(), dtype=torch.long, device=device)\n",
    "        neg_scores = model.decode(z, neg_edge_index)\n",
    "\n",
    "        y_true = torch.cat([torch.ones_like(pos_scores), torch.zeros_like(neg_scores)]).cpu().numpy()\n",
    "        y_scores = torch.cat([pos_scores, neg_scores]).cpu().numpy()\n",
    "        \n",
    "        return {\n",
    "            'auc': roc_auc_score(y_true, y_scores),\n",
    "            'ap': average_precision_score(y_true, y_scores)\n",
    "        }\n",
    "\n",
    "# ==============================================================================\n",
    "# å®Ÿé¨“å®Ÿè¡Œã¨çµæœåˆ†æ (å¤‰æ›´ãªã—)\n",
    "# ==============================================================================\n",
    "def plot_comparison_results(results, histories):\n",
    "    \"\"\"å®Ÿé¨“çµæœã‚’å¯è¦–åŒ–ã™ã‚‹\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    model_names = list(results.keys())\n",
    "    colors = ['#FF6B6B', '#4ECDC4', '#45B7D1']\n",
    "    test_aucs = [results[name]['test_auc'] for name in model_names]\n",
    "    axes[0, 0].bar(model_names, test_aucs, color=colors, alpha=0.8)\n",
    "    axes[0, 0].set_title('Test AUC Performance')\n",
    "    axes[0, 0].set_ylabel('AUC Score')\n",
    "    if all(v > 0 for v in test_aucs): axes[0, 0].set_ylim(bottom=min(test_aucs) * 0.95)\n",
    "    for i, v in enumerate(test_aucs): axes[0, 0].text(i, v, f'{v:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "    test_aps = [results[name]['test_ap'] for name in model_names]\n",
    "    axes[0, 1].bar(model_names, test_aps, color=colors, alpha=0.8)\n",
    "    axes[0, 1].set_title('Test Average Precision (AP)')\n",
    "    axes[0, 1].set_ylabel('AP Score')\n",
    "    if all(v > 0 for v in test_aps): axes[0, 1].set_ylim(bottom=min(test_aps) * 0.95)\n",
    "    for i, v in enumerate(test_aps): axes[0, 1].text(i, v, f'{v:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "    training_times = [results[name]['training_time'] for name in model_names]\n",
    "    axes[1, 0].bar(model_names, training_times, color=colors, alpha=0.8)\n",
    "    axes[1, 0].set_title('Training Time')\n",
    "    axes[1, 0].set_ylabel('Time (seconds)')\n",
    "    for i, v in enumerate(training_times): axes[1, 0].text(i, v, f'{v:.1f}s', ha='center', va='bottom', fontweight='bold')\n",
    "    for name, color in zip(model_names, colors):\n",
    "        axes[1, 1].plot(histories[name]['val_auc'], label=name, color=color, linewidth=2, marker='o', markersize=4)\n",
    "    axes[1, 1].set_title('Validation AUC during Training')\n",
    "    axes[1, 1].set_xlabel('Epoch')\n",
    "    axes[1, 1].set_ylabel('Validation AUC')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.4)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def main():\n",
    "    \"\"\"å®Ÿé¨“å…¨ä½“ã‚’ç®¡ç†ãƒ»å®Ÿè¡Œã™ã‚‹ãƒ¡ã‚¤ãƒ³é–¢æ•°\"\"\"\n",
    "    print(\"1. Loading and preprocessing data...\")\n",
    "    cleaned_df, feature_cols = preprocess_data()\n",
    "    \n",
    "    if cleaned_df.empty:\n",
    "        print(\"Error: No data loaded. Please check file paths and data availability.\")\n",
    "        return {}, {}\n",
    "    \n",
    "    graph_dict, corps, patents, num_nodes = build_global_graphs(cleaned_df, feature_cols)\n",
    "    print(f\"Data loaded: {len(corps)} corporations, {len(patents)} patents, {len(graph_dict)} yearly graphs.\")\n",
    "\n",
    "    if not graph_dict:\n",
    "        print(\"Error: No graphs built. Please check data quality.\")\n",
    "        return {}, {}\n",
    "\n",
    "    years = sorted(graph_dict.keys())\n",
    "    test_year = years[-1]\n",
    "    num_corps = len(corps)\n",
    "    \n",
    "    # ç‰¹å¾´é‡ã®æ¬¡å…ƒã‚’å‹•çš„ã«æ±ºå®š\n",
    "    input_dim = len(feature_cols) if feature_cols else 128\n",
    "    print(f\"Using input dimension: {input_dim}\")\n",
    "\n",
    "    models_to_compare = {\n",
    "        'VGAE+ODE': UnifiedVGAE(num_nodes, num_corps, input_dim=input_dim, predictor_type='ode', hidden_dim=32, latent_dim=16),\n",
    "        'VGAE+RNN': UnifiedVGAE(num_nodes, num_corps, input_dim=input_dim, predictor_type='rnn', hidden_dim=32, latent_dim=16),\n",
    "        'VGAE+LSTM': UnifiedVGAE(num_nodes, num_corps, input_dim=input_dim, predictor_type='lstm', hidden_dim=32, latent_dim=16)\n",
    "    }\n",
    "\n",
    "    results = {}\n",
    "    histories = {}\n",
    "    \n",
    "    print(\"\\n2. Starting model training and evaluation...\")\n",
    "    for name, model in models_to_compare.items():\n",
    "        start_time = time.time()\n",
    "        trained_model, history, best_val_auc = train_model(model, graph_dict, num_corps, name, num_epochs=5)\n",
    "        training_time = time.time() - start_time\n",
    "        histories[name] = history\n",
    "        try:\n",
    "            model_path = f'best_{name.lower().replace(\" \", \"_\").replace(\"+\", \"_\")}_model.pth'\n",
    "            trained_model.load_state_dict(torch.load(model_path))\n",
    "            test_result = evaluate_model(trained_model, graph_dict[test_year], num_corps)\n",
    "        except Exception as e:\n",
    "            print(f\"Could not load or evaluate model {name}: {e}\")\n",
    "            test_result = None\n",
    "        \n",
    "        results[name] = {\n",
    "            'best_val_auc': best_val_auc,\n",
    "            'test_auc': test_result['auc'] if test_result else 0,\n",
    "            'test_ap': test_result['ap'] if test_result else 0,\n",
    "            'training_time': training_time,\n",
    "            'num_params': sum(p.numel() for p in model.parameters())\n",
    "        }\n",
    "\n",
    "    print(\"\\n3. Final Results Summary:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"{'Model':<15} | {'Test AUC':<12} | {'Test AP':<12} | {'Val AUC (Best)':<18} | {'Training Time':<15} | {'Parameters':<12}\")\n",
    "    print(\"-\" * 80)\n",
    "    for name, r in results.items():\n",
    "        print(f\"{name:<15} | {r['test_auc']:<12.4f} | {r['test_ap']:<12.4f} | {r['best_val_auc']:<18.4f} | {r['training_time']:<15.1f}s | {r['num_params']:,}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    print(\"\\n4. Plotting results...\")\n",
    "    plot_comparison_results(results, histories)\n",
    "\n",
    "    best_model_name = max(results, key=lambda name: results[name]['test_auc'])\n",
    "    print(f\"\\nExperiment Conclusion: The best performing model is '{best_model_name}' with a Test AUC of {results[best_model_name]['test_auc']:.4f}.\")\n",
    "    \n",
    "    return results, histories\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    results, histories = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95048c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç°¡å˜ãªãƒ†ã‚¹ãƒˆå®Ÿè¡Œ\n",
    "print(\"Testing basic functionality...\")\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿ã®å­˜åœ¨ç¢ºèª\n",
    "print(f\"Current working directory: {os.getcwd()}\")\n",
    "print(f\"KG Dataset exists: {os.path.exists(KG_DATASET_PATH)}\")\n",
    "print(f\"Patent data exists: {os.path.exists(PATENT_DATA_PATH)}\")\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰ã®ãƒ†ã‚¹ãƒˆ\n",
    "try:\n",
    "    cleaned_df, feature_cols = preprocess_data()\n",
    "    print(f\"Data loaded successfully. Shape: {cleaned_df.shape}\")\n",
    "    print(f\"Feature columns found: {len(feature_cols)}\")\n",
    "    if not cleaned_df.empty:\n",
    "        print(f\"Sample data columns: {list(cleaned_df.columns)[:10]}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading data: {e}\")\n",
    "\n",
    "print(\"Basic test completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b3ad13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ¡ã‚¤ãƒ³å®Ÿé¨“ã®å®Ÿè¡Œï¼ˆãƒ‡ãƒ¼ã‚¿ãŒåˆ©ç”¨å¯èƒ½ãªå ´åˆã®ã¿ï¼‰\n",
    "if os.path.exists(KG_DATASET_PATH) and os.path.exists(PATENT_DATA_PATH):\n",
    "    print(\"Starting main experiment...\")\n",
    "    results, histories = main()\n",
    "else:\n",
    "    print(\"Required data files not found. Please ensure the following files exist:\")\n",
    "    print(f\"- {KG_DATASET_PATH}\")\n",
    "    print(f\"- {PATENT_DATA_PATH}\")\n",
    "    print(\"You can run the main experiment by calling main() after ensuring data is available.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10121bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.utils import negative_sampling\n",
    "from torchdiffeq import odeint\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, average_precision_score\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import japanize_matplotlib\n",
    "import os\n",
    "\n",
    "# --- è¨­å®š ---\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# --- ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆï¼ˆå®Ÿè¡Œç”¨ãƒ€ãƒŸãƒ¼ï¼‰ ---\n",
    "def generate_dummy_data():\n",
    "    \"\"\"å®Ÿé¨“ã‚’å®Ÿè¡Œã™ã‚‹ãŸã‚ã®ãƒ€ãƒŸãƒ¼CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”Ÿæˆã™ã‚‹\"\"\"\n",
    "    print(\"Dummy data not found, generating...\")\n",
    "\n",
    "    # ãƒ€ãƒŸãƒ¼ã®kg_dataset.csv\n",
    "    data_kg = {\n",
    "        'patent_number': [f'P{i:04d}' for i in range(1000)],\n",
    "        'corporation': [\n",
    "            str([f'Corp{j}' for j in np.random.choice(range(20), size=np.random.randint(1, 4), replace=False)])\n",
    "            for _ in range(1000)\n",
    "        ],\n",
    "        'year_month': [f'2015-{m:02d}' for m in range(1, 13)] * 83 + ['2016-01'] * 4,\n",
    "    }\n",
    "    kg_df = pd.DataFrame(data_kg)\n",
    "    kg_df['corporation'] = kg_df['corporation'].apply(ast.literal_eval)\n",
    "    kg_df.to_csv(\"kg_dataset.csv\", index=False)\n",
    "\n",
    "    # ãƒ€ãƒŸãƒ¼ã®kumagai_patentdata.csv\n",
    "    data_kumagai = {\n",
    "        'patent_number': [f'P{i:04d}' for i in range(1000)],\n",
    "    }\n",
    "    for i in range(64):\n",
    "        data_kumagai[f'pca_text_dim_{i}'] = np.random.rand(1000)\n",
    "        data_kumagai[f'node_dim_{i}'] = np.random.rand(1000)\n",
    "    kumagai_df = pd.DataFrame(data_kumagai)\n",
    "    kumagai_df.to_csv(\"kumagai_patentdata.csv\", index=False)\n",
    "    print(\"Dummy data generated: kg_dataset.csv and kumagai_patentdata.csv\")\n",
    "\n",
    "if not os.path.exists(\"kg_dataset.csv\") or not os.path.exists(\"kumagai_patentdata.csv\"):\n",
    "    generate_dummy_data()\n",
    "\n",
    "# --- ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ã¨ã‚°ãƒ©ãƒ•æ§‹ç¯‰ ---\n",
    "def preprocess_data():\n",
    "    \"\"\"ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†ã¨çµ±åˆã‚°ãƒ©ãƒ•ã®æ§‹ç¯‰\"\"\"\n",
    "    kg_df = pd.read_csv(\"kg_dataset.csv\")\n",
    "    kumagai_df = pd.read_csv(\"kumagai_patentdata.csv\")\n",
    "    merged_df = pd.merge(kg_df, kumagai_df, on=\"patent_number\", how=\"left\")\n",
    "\n",
    "    # NaNã®å‡¦ç†\n",
    "    cleaned_df = merged_df.dropna(subset=[\"corporation\", \"patent_number\"], how=\"any\")\n",
    "\n",
    "    # ç‰¹å¾´é‡åˆ—ã®å‡¦ç†\n",
    "    cols = [f\"pca_text_dim_{i}\" for i in range(64)] + [f\"node_dim_{i}\" for i in range(64)]\n",
    "    cleaned_df[cols] = cleaned_df[cols].apply(pd.to_numeric, errors='coerce').replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "    cleaned_df[cols] = cleaned_df[cols].astype(np.float32)\n",
    "\n",
    "    # æ–‡å­—åˆ—ãƒªã‚¹ãƒˆã‚’Pythonãƒªã‚¹ãƒˆã«å¤‰æ›\n",
    "    def safe_literal_eval(s):\n",
    "        try:\n",
    "            return ast.literal_eval(s)\n",
    "        except (ValueError, SyntaxError):\n",
    "            return []\n",
    "\n",
    "    cleaned_df[\"corporation\"] = cleaned_df[\"corporation\"].apply(safe_literal_eval)\n",
    "    cleaned_df['year_month'] = pd.to_datetime(cleaned_df['year_month'], format='%Y-%m')\n",
    "\n",
    "    return cleaned_df\n",
    "\n",
    "def build_global_graphs(cleaned_df):\n",
    "    \"\"\"ã‚°ãƒ­ãƒ¼ãƒãƒ«çµ±åˆã‚°ãƒ©ãƒ•ã®æ§‹ç¯‰\"\"\"\n",
    "    year_groups = cleaned_df.groupby(cleaned_df['year_month'].dt.year)\n",
    "\n",
    "    all_corporations = set()\n",
    "    all_patents = set()\n",
    "\n",
    "    for year, group in year_groups:\n",
    "        for corps in group['corporation']:\n",
    "            all_corporations.update(corps)\n",
    "        all_patents.update(group['patent_number'].unique())\n",
    "\n",
    "    all_corporations = sorted(list(all_corporations))\n",
    "    all_patents = sorted(list(all_patents))\n",
    "\n",
    "    global_corp_to_idx = {corp: idx for idx, corp in enumerate(all_corporations)}\n",
    "    global_patent_to_idx = {patent: idx + len(all_corporations) for idx, patent in enumerate(all_patents)}\n",
    "    total_global_nodes = len(all_corporations) + len(all_patents)\n",
    "\n",
    "    global_graph_dict = {}\n",
    "\n",
    "    for year, group in year_groups:\n",
    "        active_nodes = set()\n",
    "        edges = []\n",
    "\n",
    "        for _, row in group.iterrows():\n",
    "            for corp in row['corporation']:\n",
    "                if corp in global_corp_to_idx and row['patent_number'] in global_patent_to_idx:\n",
    "                    corp_idx = global_corp_to_idx[corp]\n",
    "                    patent_idx = global_patent_to_idx[row['patent_number']]\n",
    "                    edges.append([corp_idx, patent_idx])\n",
    "                    active_nodes.add(corp_idx)\n",
    "                    active_nodes.add(patent_idx)\n",
    "\n",
    "        if not edges:\n",
    "            continue\n",
    "\n",
    "        x = torch.randn(total_global_nodes, 128)\n",
    "\n",
    "        for _, row in group.iterrows():\n",
    "            if row['patent_number'] in global_patent_to_idx:\n",
    "                patent_idx = global_patent_to_idx[row['patent_number']]\n",
    "                pca_feat = row[[f'pca_text_dim_{i}' for i in range(64)]].values.astype(np.float32)\n",
    "                node_feat = row[[f'node_dim_{i}' for i in range(64)]].values.astype(np.float32)\n",
    "                x[patent_idx] = torch.tensor(np.concatenate([pca_feat, node_feat]), dtype=torch.float32)\n",
    "\n",
    "        edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
    "        active_mask = torch.zeros(total_global_nodes, dtype=torch.bool)\n",
    "        active_mask[list(active_nodes)] = True\n",
    "\n",
    "        global_graph_dict[year] = Data(\n",
    "            x=x,\n",
    "            edge_index=edge_index,\n",
    "            num_nodes=total_global_nodes,\n",
    "            active_mask=active_mask,\n",
    "            year=year\n",
    "        )\n",
    "\n",
    "    return global_graph_dict, all_corporations, all_patents, total_global_nodes\n",
    "\n",
    "\n",
    "# --- ãƒ¢ãƒ‡ãƒ«å®šç¾© ---\n",
    "class SharedVGAEEncoder(nn.Module):\n",
    "    \"\"\"å…¨ãƒ¢ãƒ‡ãƒ«ã§å…±æœ‰ã™ã‚‹VGAEã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼\"\"\"\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels * 2)\n",
    "        self.conv2 = GCNConv(hidden_channels * 2, hidden_channels)\n",
    "        self.conv3 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.conv_mu = GCNConv(hidden_channels, out_channels)\n",
    "        self.conv_logvar = GCNConv(hidden_channels, out_channels)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.batch_norm1 = nn.BatchNorm1d(hidden_channels * 2)\n",
    "        self.batch_norm2 = nn.BatchNorm1d(hidden_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.relu(self.batch_norm1(self.conv1(x, edge_index)))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.batch_norm2(self.conv2(x, edge_index)))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.conv3(x, edge_index))\n",
    "        return self.conv_mu(x, edge_index), self.conv_logvar(x, edge_index)\n",
    "\n",
    "class ODEFunc(nn.Module):\n",
    "    \"\"\"NeuralODEã®å¸¸å¾®åˆ†æ–¹ç¨‹å¼é–¢æ•°\"\"\"\n",
    "    def __init__(self, latent_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, latent_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, t, z):\n",
    "        return self.net(z) * 0.1\n",
    "\n",
    "class NeuralODEPredictor(nn.Module):\n",
    "    \"\"\"NeuralODEã«ã‚ˆã‚‹é€£ç¶šæ™‚é–“äºˆæ¸¬\"\"\"\n",
    "    def __init__(self, latent_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.ode_func = ODEFunc(latent_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, z_current, delta_t=1.0):\n",
    "        t_span = torch.tensor([0., delta_t], device=z_current.device)\n",
    "        z_future = odeint(self.ode_func, z_current, t_span)[-1]\n",
    "        return z_future\n",
    "\n",
    "class RNNPredictor(nn.Module):\n",
    "    \"\"\"RNNã«ã‚ˆã‚‹é›¢æ•£æ™‚é–“äºˆæ¸¬\"\"\"\n",
    "    def __init__(self, latent_dim, hidden_dim, num_layers=2):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.RNN(latent_dim, hidden_dim, num_layers, batch_first=True, dropout=0.2)\n",
    "        self.output_layer = nn.Linear(hidden_dim, latent_dim)\n",
    "\n",
    "    def forward(self, z_sequence):\n",
    "        if z_sequence.dim() == 2:\n",
    "            z_sequence = z_sequence.unsqueeze(1)\n",
    "        rnn_out, _ = self.rnn(z_sequence)\n",
    "        z_future = self.output_layer(rnn_out[:, -1, :])\n",
    "        return z_future\n",
    "\n",
    "class LSTMPredictor(nn.Module):\n",
    "    \"\"\"LSTMã«ã‚ˆã‚‹é›¢æ•£æ™‚é–“äºˆæ¸¬\"\"\"\n",
    "    def __init__(self, latent_dim, hidden_dim, num_layers=2):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(latent_dim, hidden_dim, num_layers, batch_first=True, dropout=0.2)\n",
    "        self.output_layer = nn.Linear(hidden_dim, latent_dim)\n",
    "\n",
    "    def forward(self, z_sequence):\n",
    "        if z_sequence.dim() == 2:\n",
    "            z_sequence = z_sequence.unsqueeze(1)\n",
    "        lstm_out, _ = self.lstm(z_sequence)\n",
    "        z_future = self.output_layer(lstm_out[:, -1, :])\n",
    "        return z_future\n",
    "\n",
    "class UnifiedVGAE(nn.Module):\n",
    "    \"\"\"çµ±åˆVGAEãƒ¢ãƒ‡ãƒ« - æ™‚ç³»åˆ—äºˆæ¸¬éƒ¨åˆ†ã®ã¿äº¤æ›å¯èƒ½\"\"\"\n",
    "    def __init__(self, num_nodes, num_corps, input_dim=128, hidden_dim=64,\n",
    "                 latent_dim=16, predictor_type='ode'):\n",
    "        super().__init__()\n",
    "        self.num_nodes = num_nodes\n",
    "        self.num_corps = num_corps\n",
    "        self.predictor_type = predictor_type\n",
    "\n",
    "        self.corp_embeddings = nn.Embedding(num_corps, input_dim)\n",
    "        nn.init.xavier_uniform_(self.corp_embeddings.weight)\n",
    "        self.encoder = SharedVGAEEncoder(input_dim, hidden_dim, latent_dim)\n",
    "\n",
    "        if predictor_type == 'ode':\n",
    "            self.temporal_predictor = NeuralODEPredictor(latent_dim, hidden_dim)\n",
    "        elif predictor_type == 'rnn':\n",
    "            self.temporal_predictor = RNNPredictor(latent_dim, hidden_dim)\n",
    "        elif predictor_type == 'lstm':\n",
    "            self.temporal_predictor = LSTMPredictor(latent_dim, hidden_dim)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown predictor type: {predictor_type}\")\n",
    "\n",
    "        self.link_predictor = nn.Sequential(\n",
    "            nn.Linear(latent_dim * 2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim // 2, 1)\n",
    "        )\n",
    "\n",
    "    def get_node_features(self, x, node_indices=None):\n",
    "        \"\"\"ãƒãƒ¼ãƒ‰ã®ç‰¹å¾´é‡ã‚’å–å¾—ï¼ˆä¼æ¥­ã¯å­¦ç¿’å¯èƒ½ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ï¼‰\"\"\"\n",
    "        if node_indices is not None:\n",
    "            features = x.clone()\n",
    "            corp_mask = node_indices < self.num_corps\n",
    "            if corp_mask.any():\n",
    "                corp_indices = node_indices[corp_mask]\n",
    "                features[corp_mask] = self.corp_embeddings(corp_indices)\n",
    "            return features\n",
    "        return x\n",
    "\n",
    "    def encode(self, x, edge_index, node_indices=None):\n",
    "        \"\"\"ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ï¼ˆå…¨ãƒ¢ãƒ‡ãƒ«å…±é€šï¼‰\"\"\"\n",
    "        edge_index = edge_index.long()\n",
    "        if node_indices is not None:\n",
    "            x = self.get_node_features(x, node_indices)\n",
    "        mu, logvar = self.encoder(x, edge_index)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return z, mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        if self.training:\n",
    "            std = torch.exp(0.5 * logvar)\n",
    "            eps = torch.randn_like(std)\n",
    "            return mu + eps * std\n",
    "        return mu\n",
    "\n",
    "    def decode(self, z, edge_index):\n",
    "        \"\"\"ã‚¨ãƒƒã‚¸ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«å¯¾ã™ã‚‹ãƒªãƒ³ã‚¯ç¢ºç‡äºˆæ¸¬ï¼ˆå…¨ãƒ¢ãƒ‡ãƒ«å…±é€šï¼‰\"\"\"\n",
    "        edge_index = edge_index.long()\n",
    "        z_i = z[edge_index[0]]\n",
    "        z_j = z[edge_index[1]]\n",
    "        combined = torch.cat([z_i, z_j], dim=-1)\n",
    "        return torch.sigmoid(self.link_predictor(combined)).squeeze()\n",
    "\n",
    "    def predict_future(self, z_current):\n",
    "        \"\"\"æ™‚ç³»åˆ—äºˆæ¸¬ï¼ˆãƒ¢ãƒ‡ãƒ«ã”ã¨ã«ç•°ãªã‚‹å®Ÿè£…ï¼‰\"\"\"\n",
    "        if self.predictor_type == 'ode':\n",
    "            return self.temporal_predictor(z_current)\n",
    "        else:\n",
    "            return self.temporal_predictor(z_current)\n",
    "\n",
    "# --- è©•ä¾¡ãƒ»å­¦ç¿’é–¢æ•° ---\n",
    "def evaluate_model(model, data, num_corps):\n",
    "    \"\"\"ãƒ¢ãƒ‡ãƒ«è©•ä¾¡ï¼ˆå…¨ãƒ¢ãƒ‡ãƒ«å…±é€šï¼‰\"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        node_indices = torch.arange(model.num_nodes)\n",
    "        z, _, _ = model.encode(data.x, data.edge_index, node_indices)\n",
    "        pos_edge_index = data.edge_index\n",
    "        num_pos = pos_edge_index.size(1)\n",
    "\n",
    "        if num_pos == 0:\n",
    "            return None\n",
    "\n",
    "        pos_scores = model.decode(z, pos_edge_index).cpu().numpy()\n",
    "        pos_labels = np.ones(len(pos_scores))\n",
    "\n",
    "        active_corps = torch.unique(pos_edge_index[0][pos_edge_index[0] < num_corps])\n",
    "        active_patents = torch.unique(pos_edge_index[1][pos_edge_index[1] >= num_corps])\n",
    "        if len(active_corps) == 0 or len(active_patents) == 0:\n",
    "            return None\n",
    "\n",
    "        neg_edges = []\n",
    "        pos_set = set(tuple(p.tolist()) for p in pos_edge_index.t())\n",
    "\n",
    "        attempts = 0\n",
    "        while len(neg_edges) < num_pos and attempts < num_pos * 10:\n",
    "            corp_idx = active_corps[torch.randint(len(active_corps), (1,))].item()\n",
    "            patent_idx = active_patents[torch.randint(len(active_patents), (1,))].item()\n",
    "            if (corp_idx, patent_idx) not in pos_set:\n",
    "                neg_edges.append([corp_idx, patent_idx])\n",
    "            attempts += 1\n",
    "\n",
    "        if not neg_edges:\n",
    "            return None\n",
    "\n",
    "        neg_edge_index = torch.tensor(neg_edges, dtype=torch.long).t().to(device)\n",
    "        neg_scores = model.decode(z, neg_edge_index).cpu().numpy()\n",
    "        neg_labels = np.zeros(len(neg_scores))\n",
    "\n",
    "        y_true = np.concatenate([pos_labels, neg_labels])\n",
    "        y_scores = np.concatenate([pos_scores, neg_scores])\n",
    "\n",
    "        auc = roc_auc_score(y_true, y_scores)\n",
    "        ap = average_precision_score(y_true, y_scores)\n",
    "\n",
    "        return {'auc': auc, 'ap': ap}\n",
    "\n",
    "def compute_loss(model, data_t, data_t1, num_corps, beta=0.01, pos_weight=10.0):\n",
    "    \"\"\"æå¤±è¨ˆç®—ï¼ˆå…¨ãƒ¢ãƒ‡ãƒ«å…±é€šï¼‰\"\"\"\n",
    "    node_indices = torch.arange(model.num_nodes)\n",
    "    z_t, mu_t, logvar_t = model.encode(data_t.x, data_t.edge_index, node_indices)\n",
    "    active_mask_t = data_t.active_mask\n",
    "    pos_edge_index = data_t.edge_index\n",
    "\n",
    "    active_corps = torch.unique(pos_edge_index[0][pos_edge_index[0] < num_corps])\n",
    "    active_patents = torch.unique(pos_edge_index[1][pos_edge_index[1] >= num_corps])\n",
    "\n",
    "    neg_edges = []\n",
    "    num_pos = pos_edge_index.size(1)\n",
    "    pos_set = set(tuple(p.tolist()) for p in pos_edge_index.t())\n",
    "\n",
    "    attempts = 0\n",
    "    while len(neg_edges) < num_pos and attempts < num_pos * 10:\n",
    "        corp_idx = active_corps[torch.randint(len(active_corps), (1,))].item()\n",
    "        patent_idx = active_patents[torch.randint(len(active_patents), (1,))].item()\n",
    "        if (corp_idx, patent_idx) not in pos_set:\n",
    "            neg_edges.append([corp_idx, patent_idx])\n",
    "        attempts += 1\n",
    "\n",
    "    if neg_edges:\n",
    "        neg_edge_index = torch.tensor(neg_edges, dtype=torch.long).t().to(device)\n",
    "        pos_pred = model.decode(z_t, pos_edge_index)\n",
    "        neg_pred = model.decode(z_t, neg_edge_index)\n",
    "        pos_loss = -torch.log(pos_pred + 1e-15).mean() * pos_weight\n",
    "        neg_loss = -torch.log(1 - neg_pred + 1e-15).mean()\n",
    "        recon_loss_t = pos_loss + neg_loss\n",
    "    else:\n",
    "        recon_loss_t = torch.tensor(0.0, device=device)\n",
    "\n",
    "    kl_loss = -0.5 * torch.mean(\n",
    "        1 + logvar_t[active_mask_t] - mu_t[active_mask_t].pow(2) - logvar_t[active_mask_t].exp()\n",
    "    )\n",
    "    kl_loss = torch.clamp(kl_loss, max=10.0)\n",
    "\n",
    "    z_t1_pred = model.predict_future(z_t)\n",
    "    z_t1_true, mu_t1, _ = model.encode(data_t1.x, data_t1.edge_index, node_indices)\n",
    "\n",
    "    active_mask_t1 = data_t1.active_mask\n",
    "    if active_mask_t1.sum() > 0:\n",
    "        latent_pred_loss = F.mse_loss(z_t1_pred[active_mask_t1], mu_t1[active_mask_t1])\n",
    "    else:\n",
    "        latent_pred_loss = torch.tensor(0.0, device=device)\n",
    "\n",
    "    total_loss = recon_loss_t + beta * kl_loss + 0.5 * latent_pred_loss\n",
    "\n",
    "    return total_loss, {\n",
    "        'recon_loss': recon_loss_t.item() if torch.is_tensor(recon_loss_t) else 0.0,\n",
    "        'kl_loss': kl_loss.item(),\n",
    "        'latent_pred_loss': latent_pred_loss.item() if torch.is_tensor(latent_pred_loss) else 0.0,\n",
    "        'total_loss': total_loss.item()\n",
    "    }\n",
    "\n",
    "def train_model(model, global_graph_dict, num_corps, model_name, num_epochs=40):\n",
    "    \"\"\"ãƒ¢ãƒ‡ãƒ«å­¦ç¿’\"\"\"\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, patience=8, factor=0.5)\n",
    "\n",
    "    years = sorted(global_graph_dict.keys())\n",
    "    if len(years) < 3:\n",
    "        print(\"ãƒ‡ãƒ¼ã‚¿ãŒå°‘ãªã™ãã¾ã™ã€‚æœ€ä½3å¹´åˆ†ã®ãƒ‡ãƒ¼ã‚¿ãŒå¿…è¦ã§ã™ã€‚\")\n",
    "        return None, None, 0\n",
    "\n",
    "    train_years = years[:-2]\n",
    "    val_year = years[-2]\n",
    "\n",
    "    history = {'train_loss': [], 'train_auc': [], 'val_auc': [], 'latent_pred_loss': []}\n",
    "    best_val_auc = 0\n",
    "    patience_counter = 0\n",
    "\n",
    "    print(f\"\\n=== {model_name} å­¦ç¿’é–‹å§‹ ===\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_losses = []\n",
    "\n",
    "        for i in range(len(train_years) - 1):\n",
    "            year_t = train_years[i]\n",
    "            year_t1 = train_years[i + 1]\n",
    "\n",
    "            data_t = global_graph_dict[year_t].to(device)\n",
    "            data_t1 = global_graph_dict[year_t1].to(device)\n",
    "\n",
    "            if data_t.edge_index.size(1) == 0 or data_t1.edge_index.size(1) == 0:\n",
    "                continue\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            try:\n",
    "                loss, loss_dict = compute_loss(model, data_t, data_t1, num_corps)\n",
    "                if not torch.isnan(loss):\n",
    "                    loss.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                    optimizer.step()\n",
    "                    epoch_losses.append(loss_dict)\n",
    "            except Exception as e:\n",
    "                print(f\"Error at year {year_t} -> {year_t1}: {e}\")\n",
    "                continue\n",
    "\n",
    "        if epoch % 5 == 0 and epoch_losses:\n",
    "            avg_loss = np.mean([l['total_loss'] for l in epoch_losses])\n",
    "            avg_pred_loss = np.mean([l['latent_pred_loss'] for l in epoch_losses])\n",
    "            history['train_loss'].append(avg_loss)\n",
    "            history['latent_pred_loss'].append(avg_pred_loss)\n",
    "\n",
    "            train_result = evaluate_model(model, global_graph_dict[train_years[-1]].to(device), num_corps)\n",
    "            val_result = evaluate_model(model, global_graph_dict[val_year].to(device), num_corps)\n",
    "\n",
    "            train_auc = train_result['auc'] if train_result else None\n",
    "            val_auc = val_result['auc'] if val_result else None\n",
    "\n",
    "            if train_auc: history['train_auc'].append(train_auc)\n",
    "            if val_auc: history['val_auc'].append(val_auc)\n",
    "\n",
    "            train_auc_str = f\"{train_auc:.3f}\" if train_auc else \"N/A\"\n",
    "            val_auc_str = f\"{val_auc:.3f}\" if val_auc else \"N/A\"\n",
    "\n",
    "            print(f\"Epoch {epoch:2d}: Loss={avg_loss:.4f}, PredLoss={avg_pred_loss:.4f}, \"\n",
    "                  f\"TrainAUC={train_auc_str}, ValAUC={val_auc_str}\")\n",
    "\n",
    "            if val_auc and val_auc > best_val_auc:\n",
    "                best_val_auc = val_auc\n",
    "                patience_counter = 0\n",
    "                torch.save(model.state_dict(), f'best_{model_name.lower().replace(\" \", \"_\").replace(\"+\", \"_\")}_model.pth')\n",
    "                print(f\"âœ… Best model saved! Val AUC: {val_auc:.3f}\")\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "\n",
    "            scheduler.step(avg_loss)\n",
    "\n",
    "            if patience_counter >= 15:\n",
    "                print(\"Early stopping triggered\")\n",
    "                break\n",
    "    return model, history, best_val_auc\n",
    "\n",
    "def compare_temporal_predictors():\n",
    "    \"\"\"æ™‚ç³»åˆ—äºˆæ¸¬å™¨ã®å…¬å¹³ãªæ¯”è¼ƒå®Ÿé¨“\"\"\"\n",
    "    print(\"ğŸ”¬ æ™‚ç³»åˆ—äºˆæ¸¬å™¨ã®æ¯”è¼ƒå®Ÿé¨“\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"ç›®çš„: VGAEã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã‚’çµ±ä¸€ã—ã€æ™‚ç³»åˆ—äºˆæ¸¬éƒ¨åˆ†ã®ã¿ã®æ€§èƒ½å·®ã‚’æ¸¬å®š\")\n",
    "    print(\"æ¯”è¼ƒå¯¾è±¡:\")\n",
    "    print(\"1. VGAE + NeuralODE (é€£ç¶šæ™‚é–“å‹•åŠ›å­¦)\")\n",
    "    print(\"2. VGAE + RNN (é›¢æ•£æ™‚ç³»åˆ—)\")\n",
    "    print(\"3. VGAE + LSTM (å¼·åŠ›ãªé›¢æ•£æ™‚ç³»åˆ—)\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    print(\"\\nğŸ“Š ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†...\")\n",
    "    cleaned_df = preprocess_data()\n",
    "    global_graph_dict, all_corporations, all_patents, total_global_nodes = build_global_graphs(cleaned_df)\n",
    "\n",
    "    if len(global_graph_dict) < 3:\n",
    "        print(\"å®Ÿé¨“ã«å¿…è¦ãªååˆ†ãªãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚\")\n",
    "        return {}, {}\n",
    "\n",
    "    print(f\"å…¨ä¼æ¥­æ•°: {len(all_corporations)}\")\n",
    "    print(f\"å…¨ç‰¹è¨±æ•°: {len(all_patents)}\")\n",
    "    print(f\"å…¨ãƒãƒ¼ãƒ‰æ•°: {total_global_nodes}\")\n",
    "    print(f\"ã‚°ãƒ©ãƒ•æ•°: {len(global_graph_dict)}\")\n",
    "\n",
    "    years = sorted(global_graph_dict.keys())\n",
    "    test_year = years[-1]\n",
    "\n",
    "    models = {\n",
    "        'VGAE + NeuralODE': UnifiedVGAE(\n",
    "            num_nodes=total_global_nodes, num_corps=len(all_corporations),\n",
    "            input_dim=128, hidden_dim=64, latent_dim=16, predictor_type='ode'\n",
    "        ),\n",
    "        'VGAE + RNN': UnifiedVGAE(\n",
    "            num_nodes=total_global_nodes, num_corps=len(all_corporations),\n",
    "            input_dim=128, hidden_dim=64, latent_dim=16, predictor_type='rnn'\n",
    "        ),\n",
    "        'VGAE + LSTM': UnifiedVGAE(\n",
    "            num_nodes=total_global_nodes, num_corps=len(all_corporations),\n",
    "            input_dim=128, hidden_dim=64, latent_dim=16, predictor_type='lstm'\n",
    "        )\n",
    "    }\n",
    "\n",
    "    print(\"\\nğŸ“ˆ ãƒ¢ãƒ‡ãƒ«è¤‡é›‘åº¦æ¯”è¼ƒ:\")\n",
    "    for name, model in models.items():\n",
    "        num_params = sum(p.numel() for p in model.parameters())\n",
    "        predictor_params = sum(p.numel() for p in model.temporal_predictor.parameters())\n",
    "        print(f\"{name}: ç·ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°={num_params:,}, äºˆæ¸¬å™¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°={predictor_params:,}\")\n",
    "\n",
    "    results = {}\n",
    "    histories = {}\n",
    "\n",
    "    for name, model in models.items():\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"ğŸš€ {name} ã®å­¦ç¿’ãƒ»è©•ä¾¡\")\n",
    "        print(f\"{'='*60}\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        trained_model, history, best_val_auc = train_model(\n",
    "            model, global_graph_dict, len(all_corporations), name, num_epochs=40\n",
    "        )\n",
    "        training_time = time.time() - start_time\n",
    "\n",
    "        if trained_model is None:\n",
    "            continue\n",
    "\n",
    "        model_path = f'best_{name.lower().replace(\" \", \"_\").replace(\"+\", \"_\")}_model.pth'\n",
    "        if os.path.exists(model_path):\n",
    "            trained_model.load_state_dict(torch.load(model_path))\n",
    "            print(f\"âœ… ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰: {model_path}\")\n",
    "        else:\n",
    "            print(f\"âš ï¸ ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ç¾åœ¨ã®ãƒ¢ãƒ‡ãƒ«ã§ãƒ†ã‚¹ãƒˆè©•ä¾¡ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚\")\n",
    "\n",
    "        test_result = evaluate_model(trained_model, global_graph_dict[test_year].to(device), len(all_corporations))\n",
    "\n",
    "        results[name] = {\n",
    "            'best_val_auc': best_val_auc,\n",
    "            'test_auc': test_result['auc'] if test_result else 0.50,\n",
    "            'test_ap': test_result['ap'] if test_result else 0.50,\n",
    "            'training_time': training_time,\n",
    "            'num_params': sum(p.numel() for p in model.parameters()),\n",
    "            'predictor_params': sum(p.numel() for p in model.temporal_predictor.parameters())\n",
    "        }\n",
    "        histories[name] = history\n",
    "\n",
    "        print(f\"\\nğŸ“Š {name} æœ€çµ‚çµæœ:\")\n",
    "        print(f\"  Best Validation AUC: {results[name]['best_val_auc']:.3f}\")\n",
    "        print(f\"  Test AUC: {results[name]['test_auc']:.3f}\")\n",
    "        print(f\"  Test Average Precision: {results[name]['test_ap']:.3f}\")\n",
    "        print(f\"  å­¦ç¿’æ™‚é–“: {training_time:.1f}ç§’\")\n",
    "\n",
    "    return results, histories\n",
    "\n",
    "def analyze_temporal_modeling_differences(results, histories):\n",
    "    \"\"\"æ™‚ç³»åˆ—ãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã®é•ã„ã‚’è©³ç´°åˆ†æ\"\"\"\n",
    "    if not results:\n",
    "        return\n",
    "\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"ğŸ” æ™‚ç³»åˆ—ãƒ¢ãƒ‡ãƒªãƒ³ã‚°æ‰‹æ³•ã®è©³ç´°æ¯”è¼ƒåˆ†æ\")\n",
    "    print(f\"{'='*80}\")\n",
    "\n",
    "    # 1. æ€§èƒ½æ¯”è¼ƒè¡¨\n",
    "    print(\"\\n1ï¸âƒ£ æ€§èƒ½æ¯”è¼ƒã‚µãƒãƒªãƒ¼\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"{'æ‰‹æ³•':<20} {'Test AUC':<12} {'Test AP':<12} {'Val AUC':<12} {'å­¦ç¿’æ™‚é–“(ç§’)':<15} {'äºˆæ¸¬å™¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°':<20}\")\n",
    "    print(\"-\" * 80)\n",
    "    for name in results:\n",
    "        r = results[name]\n",
    "        print(f\"{name:<20} {r['test_auc']:.3f} {r['test_ap']:.3f} {r['best_val_auc']:.3f} {r['training_time']:.1f} {r['predictor_params']:,}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    # 2. ç›¸å¯¾çš„æ”¹å–„åº¦åˆ†æ\n",
    "    print(f\"\\n2ï¸âƒ£ ç›¸å¯¾çš„æ”¹å–„åº¦åˆ†æ (VGAE + RNNã‚’åŸºæº–)\")\n",
    "    print(\"-\" * 80)\n",
    "    if 'VGAE + RNN' in results:\n",
    "        rnn_auc = results['VGAE + RNN']['test_auc']\n",
    "        rnn_ap = results['VGAE + RNN']['test_ap']\n",
    "\n",
    "        for name in results:\n",
    "            if name != 'VGAE + RNN':\n",
    "                auc_imp = ((results[name]['test_auc'] - rnn_auc) / rnn_auc) * 100\n",
    "                ap_imp = ((results[name]['test_ap'] - rnn_ap) / rnn_ap) * 100\n",
    "                print(f\"  {name} vs RNN: Test AUC {auc_imp:+.2f}%æ”¹å–„, Test AP {ap_imp:+.2f}%æ”¹å–„\")\n",
    "\n",
    "    # 3. è¨ˆç®—åŠ¹ç‡æ€§åˆ†æ\n",
    "    print(f\"\\n3ï¸âƒ£ è¨ˆç®—åŠ¹ç‡æ€§åˆ†æ\")\n",
    "    print(\"-\" * 80)\n",
    "    for name in results:\n",
    "        r = results[name]\n",
    "        auc_efficiency = r['test_auc'] / (r['predictor_params'] / 1000)\n",
    "        time_efficiency = r['test_auc'] / (r['training_time'] / 60)\n",
    "        print(f\"  {name}: AUC/1K params={auc_efficiency:.4f}, AUC/min={time_efficiency:.4f}\")\n",
    "\n",
    "    # 4. å¯è¦–åŒ–\n",
    "    plot_temporal_comparison(results, histories)\n",
    "\n",
    "    # 5. æ™‚ç³»åˆ—äºˆæ¸¬ã®è³ªçš„åˆ†æ\n",
    "    print(f\"\\n4ï¸âƒ£ æ™‚ç³»åˆ—äºˆæ¸¬ã®è³ªçš„åˆ†æ\")\n",
    "    print(\"-\" * 80)\n",
    "    print(\"ğŸ”¸ NeuralODEã®ç‰¹å¾´:\")\n",
    "    print(\"  - é€£ç¶šçš„ã§æ»‘ã‚‰ã‹ãªæ™‚é–“ç™ºå±•ã‚’ãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã§ãã€ä»»æ„ã®æ™‚é–“é–“éš”ã§ã®äºˆæ¸¬ãŒå¯èƒ½ã€‚\")\n",
    "    print(\"  - å¾®åˆ†æ–¹ç¨‹å¼ã§è¡¨ç¾ã•ã‚Œã‚‹ãŸã‚ã€ç‰©ç†çš„ãªãƒ€ã‚¤ãƒŠãƒŸã‚¯ã‚¹ã®è§£é‡ˆã«å„ªã‚Œã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ã€‚\")\n",
    "    print(\"  - ODEã‚½ãƒ«ãƒãƒ¼ã«ã‚ˆã‚‹æ•°å€¤è§£æ³•ã®ãŸã‚ã€è¨ˆç®—ã‚³ã‚¹ãƒˆãŒå¤‰å‹•ã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ã€‚\")\n",
    "\n",
    "    print(f\"\\nğŸ”¸ RNN/LSTMã®ç‰¹å¾´:\")\n",
    "    print(\"  - é›¢æ•£çš„ãªæ™‚é–“ã‚¹ãƒ†ãƒƒãƒ—ã§ã®ãƒ‘ã‚¿ãƒ¼ãƒ³å­¦ç¿’ã«ç‰¹åŒ–ã—ã¦ã„ã‚‹ã€‚\")\n",
    "    print(\"  - é•·æœŸçš„ãªä¾å­˜é–¢ä¿‚ã‚’å­¦ç¿’ã™ã‚‹èƒ½åŠ›ã«å„ªã‚Œã‚‹ï¼ˆç‰¹ã«LSTMï¼‰ã€‚\")\n",
    "    print(\"  - å›ºå®šã•ã‚ŒãŸæ™‚é–“ã‚¹ãƒ†ãƒƒãƒ—ã«ä¾å­˜ã—ã€è¦³æ¸¬é »åº¦ãŒä¸è¦å‰‡ãªãƒ‡ãƒ¼ã‚¿ã«ã¯é©ç”¨ã—ã«ãã„ã€‚\")\n",
    "\n",
    "\n",
    "def plot_temporal_comparison(results, histories):\n",
    "    \"\"\"æ™‚ç³»åˆ—äºˆæ¸¬æ‰‹æ³•ã®æ¯”è¼ƒå¯è¦–åŒ–\"\"\"\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "\n",
    "    model_names = list(results.keys())\n",
    "    colors = ['#FF6B6B', '#4ECDC4', '#45B7D1'] if len(model_names) == 3 else plt.cm.viridis(np.linspace(0, 1, len(model_names)))\n",
    "\n",
    "    # 1. Test AUCæ¯”è¼ƒ\n",
    "    test_aucs = [results[name]['test_auc'] for name in model_names]\n",
    "    axes[0, 0].bar(range(len(model_names)), test_aucs, color=colors, alpha=0.8)\n",
    "    axes[0, 0].set_xlabel('æ™‚ç³»åˆ—äºˆæ¸¬æ‰‹æ³•')\n",
    "    axes[0, 0].set_ylabel('Test AUC')\n",
    "    axes[0, 0].set_title('ğŸ¯ Test AUCæ€§èƒ½æ¯”è¼ƒ')\n",
    "    axes[0, 0].set_xticks(range(len(model_names)))\n",
    "    axes[0, 0].set_xticklabels([name.split(' + ')[1] for name in model_names])\n",
    "    for i, auc in enumerate(test_aucs):\n",
    "        axes[0, 0].text(i, auc + 0.005, f'{auc:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "    # 2. Average Precisionæ¯”è¼ƒ\n",
    "    test_aps = [results[name]['test_ap'] for name in model_names]\n",
    "    axes[0, 1].bar(range(len(model_names)), test_aps, color=colors, alpha=0.8)\n",
    "    axes[0, 1].set_xlabel('æ™‚ç³»åˆ—äºˆæ¸¬æ‰‹æ³•')\n",
    "    axes[0, 1].set_ylabel('Test Average Precision')\n",
    "    axes[0, 1].set_title('ğŸ“Š Average Precisionæ¯”è¼ƒ')\n",
    "    axes[0, 1].set_xticks(range(len(model_names)))\n",
    "    axes[0, 1].set_xticklabels([name.split(' + ')[1] for name in model_names])\n",
    "    for i, ap in enumerate(test_aps):\n",
    "        axes[0, 1].text(i, ap + 0.005, f'{ap:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "    # 3. å­¦ç¿’æ™‚é–“æ¯”è¼ƒ\n",
    "    training_times = [results[name]['training_time'] for name in model_names]\n",
    "    axes[0, 2].bar(range(len(model_names)), training_times, color=colors, alpha=0.8)\n",
    "    axes[0, 2].set_xlabel('æ™‚ç³»åˆ—äºˆæ¸¬æ‰‹æ³•')\n",
    "    axes[0, 2].set_ylabel('å­¦ç¿’æ™‚é–“ (ç§’)')\n",
    "    axes[0, 2].set_title('â±ï¸ å­¦ç¿’æ™‚é–“æ¯”è¼ƒ')\n",
    "    axes[0, 2].set_xticks(range(len(model_names)))\n",
    "    axes[0, 2].set_xticklabels([name.split(' + ')[1] for name in model_names])\n",
    "    for i, time_val in enumerate(training_times):\n",
    "        axes[0, 2].text(i, time_val + max(training_times)*0.02, f'{time_val:.1f}s', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "    # 4. äºˆæ¸¬å™¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°æ¯”è¼ƒ\n",
    "    predictor_params = [results[name]['predictor_params'] for name in model_names]\n",
    "    axes[1, 0].bar(range(len(model_names)), predictor_params, color=colors, alpha=0.8)\n",
    "    axes[1, 0].set_xlabel('æ™‚ç³»åˆ—äºˆæ¸¬æ‰‹æ³•')\n",
    "    axes[1, 0].set_ylabel('äºˆæ¸¬å™¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°')\n",
    "    axes[1, 0].set_title('ğŸ”§ ãƒ¢ãƒ‡ãƒ«è¤‡é›‘åº¦æ¯”è¼ƒ')\n",
    "    axes[1, 0].set_xticks(range(len(model_names)))\n",
    "    axes[1, 0].set_xticklabels([name.split(' + ')[1] for name in model_names])\n",
    "    for i, params in enumerate(predictor_params):\n",
    "        axes[1, 0].text(i, params + max(predictor_params)*0.02, f'{params:,}', ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "\n",
    "    # 5. å­¦ç¿’æ›²ç·šæ¯”è¼ƒï¼ˆValidation AUCï¼‰\n",
    "    for i, (name, color) in enumerate(zip(model_names, colors)):\n",
    "        history = histories[name]\n",
    "        if history['val_auc']:\n",
    "            epochs = list(range(0, len(history['val_auc']) * 5, 5))\n",
    "            axes[1, 1].plot(epochs, history['val_auc'], marker='o', label=name.split(' + ')[1], linewidth=2, color=color, markersize=6)\n",
    "    axes[1, 1].set_xlabel('Epoch')\n",
    "    axes[1, 1].set_ylabel('Validation AUC')\n",
    "    axes[1, 1].set_title('ğŸ“ˆ å­¦ç¿’æ›²ç·šæ¯”è¼ƒ')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "    # 6. åŠ¹ç‡æ€§åˆ†æï¼ˆAUC vs ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ï¼‰\n",
    "    for i, (name, color) in enumerate(zip(model_names, colors)):\n",
    "        x = predictor_params[i]\n",
    "        y = test_aucs[i]\n",
    "        axes[1, 2].scatter(x, y, s=150, color=color, alpha=0.8, edgecolors='black', linewidth=2)\n",
    "        axes[1, 2].annotate(name.split(' + ')[1], (x, y), xytext=(10, 10), textcoords='offset points', fontsize=10, fontweight='bold')\n",
    "    axes[1, 2].set_xlabel('äºˆæ¸¬å™¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°')\n",
    "    axes[1, 2].set_ylabel('Test AUC')\n",
    "    axes[1, 2].set_title('âš¡ åŠ¹ç‡æ€§åˆ†æ (æ€§èƒ½ vs è¤‡é›‘åº¦)')\n",
    "    axes[1, 2].grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def main():\n",
    "    \"\"\"ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°\"\"\"\n",
    "    print(\"ğŸ§ª VGAEæ™‚ç³»åˆ—äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ã®æ¯”è¼ƒå®Ÿé¨“\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"ç›®çš„: VGAEã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã‚’å…±æœ‰ã—ã€æ™‚ç³»åˆ—äºˆæ¸¬æ‰‹æ³•(NeuralODE, RNN, LSTM)ã®æ€§èƒ½ã‚’æ¯”è¼ƒã—ã¾ã™ã€‚\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    results, histories = compare_temporal_predictors()\n",
    "\n",
    "    if results:\n",
    "        analyze_temporal_modeling_differences(results, histories)\n",
    "\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"ğŸ¯ å®Ÿé¨“çµè«–\")\n",
    "        print(f\"{'='*80}\")\n",
    "\n",
    "        sorted_results = sorted(results.items(), key=lambda x: x[1]['test_auc'], reverse=True)\n",
    "\n",
    "        print(\"ğŸ” ä¸»è¦ãªç™ºè¦‹:\")\n",
    "        print(f\"1. æœ€é«˜æ€§èƒ½ãƒ¢ãƒ‡ãƒ«: {sorted_results[0][0]} (Test AUC: {sorted_results[0][1]['test_auc']:.3f})\")\n",
    "        print(\"2. é€£ç¶švsé›¢æ•£ãƒ¢ãƒ‡ãƒªãƒ³ã‚°: æ™‚ç³»åˆ—äºˆæ¸¬ã®æ‰‹æ³•ãŒã€ç‰¹è¨±ãƒªãƒ³ã‚¯äºˆæ¸¬ã®ç²¾åº¦ã«æ˜ç¢ºãªå½±éŸ¿ã‚’ä¸ãˆã¾ã—ãŸã€‚\")\n",
    "        print(\"3. è¨ˆç®—ã‚³ã‚¹ãƒˆã¨æ€§èƒ½ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•: NeuralODEã¯é«˜ã„æ€§èƒ½ã‚’ç¤ºã™ä¸€æ–¹ã€å­¦ç¿’æ™‚é–“ã‚„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŠ¹ç‡ã«é•ã„ãŒè¦‹ã‚‰ã‚Œã¾ã—ãŸã€‚\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
